{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Compliant Kubernetes Compliant Kubernetes is a Certified Kubernetes distribution, i.e., an opinionated way of packaging and configuring Kubernetes together with other projects. Compliant Kubernetes reduces the compliance burden, as required to comply with: Health Insurance Portability and Accountability Act (HIPAA) Swedish Healthcare (Patientdatalagen) General Data Protection Regulation (GDPR) Payment Card Industry Data Security Standard (PCI DSS) Finansinspektionen's Regulatory Code (e.g., FFFS 2014:7) Other regulations that map to information security standards, such as ISO 27001 Why Compliant Kubernetes? Kubernetes has established itself as a go-to solution for high development velocity without vendor lock-in. However, vanilla Kubernetes is not usable in regulated industry, since it is not secure by default, nor by itself . Therefore, if you want to benefit from the speed of cloud native development in regulated industries, Kubernetes needs to be carefully configured. Furthermore, Kubernetes is a laser-focused project (\"Make each program do one thing well.\"), so it needs to be complemented with other cloud native projects. Compliant Kubernetes fills this gap. Architecture Below we present the architecture of Compliant Kubernetes, using the C4 model . Level 1: System Context Let us start with the system context. Compliance imposes restrictions on all levels of the teck stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware. Level 2: Clusters Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack. To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters A workload cluster , which hosts your application, and A service cluster , which hosts services for monitoring, logging and vulnerability management. Note Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API . Currently, these components are: Falco, for intrusion detection; Prometheus, for collecting metrics; Fluentd, for collecting logs; OpenPolicyAgent, for enforcing Kubernetes API policies. Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.","title":"Overview"},{"location":"#welcome-to-compliant-kubernetes","text":"Compliant Kubernetes is a Certified Kubernetes distribution, i.e., an opinionated way of packaging and configuring Kubernetes together with other projects. Compliant Kubernetes reduces the compliance burden, as required to comply with: Health Insurance Portability and Accountability Act (HIPAA) Swedish Healthcare (Patientdatalagen) General Data Protection Regulation (GDPR) Payment Card Industry Data Security Standard (PCI DSS) Finansinspektionen's Regulatory Code (e.g., FFFS 2014:7) Other regulations that map to information security standards, such as ISO 27001","title":"Welcome to Compliant Kubernetes"},{"location":"#why-compliant-kubernetes","text":"Kubernetes has established itself as a go-to solution for high development velocity without vendor lock-in. However, vanilla Kubernetes is not usable in regulated industry, since it is not secure by default, nor by itself . Therefore, if you want to benefit from the speed of cloud native development in regulated industries, Kubernetes needs to be carefully configured. Furthermore, Kubernetes is a laser-focused project (\"Make each program do one thing well.\"), so it needs to be complemented with other cloud native projects. Compliant Kubernetes fills this gap.","title":"Why Compliant Kubernetes?"},{"location":"#architecture","text":"Below we present the architecture of Compliant Kubernetes, using the C4 model .","title":"Architecture"},{"location":"#level-1-system-context","text":"Let us start with the system context. Compliance imposes restrictions on all levels of the teck stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware.","title":"Level 1: System Context"},{"location":"#level-2-clusters","text":"Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack. To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters A workload cluster , which hosts your application, and A service cluster , which hosts services for monitoring, logging and vulnerability management. Note Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API . Currently, these components are: Falco, for intrusion detection; Prometheus, for collecting metrics; Fluentd, for collecting logs; OpenPolicyAgent, for enforcing Kubernetes API policies. Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.","title":"Level 2: Clusters"},{"location":"benefits/","text":"Exploring the benefits of Compliant Kubernetes If you are new to the Compliant Kubernetes project, you are encouraged to carry out your own proof-of-concept using Compliant Kubernetes to get a feel for its features and how it can provide tangible benefit to your application and operations. The following are suggested aspects to investigate, which also help building an understanding for the platform. The application As a Kubernetes-based platform distribution of software, any containerized application will of course do. However, to get the best possible understanding of the Compliant Kubernetes platform's features, we suggest that your application has: a publicly facing front end part, which connects to a back end business logic application, which connects to a database system. This will let you explore some of the features that the Compliant Kubernetes platform offers and see how these benefit your needs and workflows. Beware: PodSecurityPolicies in place Be mindful of not trying to start Pods that assume they can run using the root account. In regulated environments, doing this should of course not be permitted, as it needlessly increases your attack surface. So all your applications should run with as few permissions as possible. Add capabilities if you must, but don't try to run as root! Compliant Kubernetes benefits to explore Integration with your identity provider (IdP) of choice Since Compliant Kubernetes relies on Dex , it integrates with various identity providers, such as LDAP-based ones (Active Directory), SAML, OpenID Connect, GitHub, and more, including Google accounts. See the whole list of support by looking at the list of connectors . Set it up with your provider of choice to get a feel for how easily it will integrate with your workflow! Application and audit logs to Elasticsearch Your application just needs to write to standard output, as is typical in Kubernetes-based platform. Whatever it writes will be collected and stored in your preconfigured Elasticsearch service that Compliant Kubernetes ships with (OpenDistro for Elasticsearch). Access it from your dashboard! Check out the audit logs as well, to see that all API actions against the Kubernetes API are logged in Elasticsearch. Perhaps try to carry out administrative tasks with a non-privileged user to see that these are prevented and logged. You can of course set up alerting for this type of event. Monitoring data to Prometheus, viewed with Grafana The Pods of your application will automatically be monitored by Prometheus. Check it out from your dashboard and see the data update. Custom incident alerts in Alertmanager Create a couple of alerts that make sense for your application based on the data reported into Prometheus. Alertmanager integrates natively with e.g. Slack and PagerDuty and also supports a wide range of additional notification channels via webhooks so you can try setting up something that makes sense for your operations team. Image vulnerability scanning for known threats Upload your container images to the Compliant Kubernetes Harbor image registry. Verify that scanning takes place, and see if your images are secure. How about intentionally pushing an image based on some old base image to see the list populate fast! Intrusion detection system for unknown threats Make your application do unsafe things to trigger Falco, the intrusion detection system. Try to write something to the /etc directory! Policies and automatic enforcement Compliant Kubernetes integrates with the Open Policy Agent (OPA), which helps enforce policies automatically. Such policies can prevent e.g. the use of default passwords when connecting to databases, or configuration errors such as deploying a non-vetted Pod to production. Set up a policy that makes sense for your application and watch as OPA immediately stops violations to these policies to occur. It catches API requests to the Kubernetes API before they can touch resources in the cluster. Network isolation/segregation Set up standard Kubernetes Network Policies such that: only the front end is publicly exposed; the front end can only initiate connections to the back end; the back end can only be connected to from the front end (no other components running in other namespaces); the back end only gets to initiate connections to the database; nothing besides the back end application gets to connect to the database; and the database may never initiate connections to anything on its own. Doing this shows that not only do you have network isolation/segregation up and running, but also, you have significantly reduced your attack surface. Should code get exploited in either component, it will still be limited in what damage it can do to the overall system. Automatic certificate management Of course your publicly exposed front end should support (only?) encrypted traffic. Set up the cert-manager to give your exposed service a certificate issued by Let's Encrypt.","title":"Exploring Benefits"},{"location":"benefits/#exploring-the-benefits-of-compliant-kubernetes","text":"If you are new to the Compliant Kubernetes project, you are encouraged to carry out your own proof-of-concept using Compliant Kubernetes to get a feel for its features and how it can provide tangible benefit to your application and operations. The following are suggested aspects to investigate, which also help building an understanding for the platform.","title":"Exploring the benefits of Compliant Kubernetes"},{"location":"benefits/#the-application","text":"As a Kubernetes-based platform distribution of software, any containerized application will of course do. However, to get the best possible understanding of the Compliant Kubernetes platform's features, we suggest that your application has: a publicly facing front end part, which connects to a back end business logic application, which connects to a database system. This will let you explore some of the features that the Compliant Kubernetes platform offers and see how these benefit your needs and workflows.","title":"The application"},{"location":"benefits/#beware-podsecuritypolicies-in-place","text":"Be mindful of not trying to start Pods that assume they can run using the root account. In regulated environments, doing this should of course not be permitted, as it needlessly increases your attack surface. So all your applications should run with as few permissions as possible. Add capabilities if you must, but don't try to run as root!","title":"Beware: PodSecurityPolicies in place"},{"location":"benefits/#compliant-kubernetes-benefits-to-explore","text":"","title":"Compliant Kubernetes benefits to explore"},{"location":"benefits/#integration-with-your-identity-provider-idp-of-choice","text":"Since Compliant Kubernetes relies on Dex , it integrates with various identity providers, such as LDAP-based ones (Active Directory), SAML, OpenID Connect, GitHub, and more, including Google accounts. See the whole list of support by looking at the list of connectors . Set it up with your provider of choice to get a feel for how easily it will integrate with your workflow!","title":"Integration with your identity provider (IdP) of choice"},{"location":"benefits/#application-and-audit-logs-to-elasticsearch","text":"Your application just needs to write to standard output, as is typical in Kubernetes-based platform. Whatever it writes will be collected and stored in your preconfigured Elasticsearch service that Compliant Kubernetes ships with (OpenDistro for Elasticsearch). Access it from your dashboard! Check out the audit logs as well, to see that all API actions against the Kubernetes API are logged in Elasticsearch. Perhaps try to carry out administrative tasks with a non-privileged user to see that these are prevented and logged. You can of course set up alerting for this type of event.","title":"Application and audit logs to Elasticsearch"},{"location":"benefits/#monitoring-data-to-prometheus-viewed-with-grafana","text":"The Pods of your application will automatically be monitored by Prometheus. Check it out from your dashboard and see the data update.","title":"Monitoring data to Prometheus, viewed with Grafana"},{"location":"benefits/#custom-incident-alerts-in-alertmanager","text":"Create a couple of alerts that make sense for your application based on the data reported into Prometheus. Alertmanager integrates natively with e.g. Slack and PagerDuty and also supports a wide range of additional notification channels via webhooks so you can try setting up something that makes sense for your operations team.","title":"Custom incident alerts in Alertmanager"},{"location":"benefits/#image-vulnerability-scanning-for-known-threats","text":"Upload your container images to the Compliant Kubernetes Harbor image registry. Verify that scanning takes place, and see if your images are secure. How about intentionally pushing an image based on some old base image to see the list populate fast!","title":"Image vulnerability scanning for known threats"},{"location":"benefits/#intrusion-detection-system-for-unknown-threats","text":"Make your application do unsafe things to trigger Falco, the intrusion detection system. Try to write something to the /etc directory!","title":"Intrusion detection system for unknown threats"},{"location":"benefits/#policies-and-automatic-enforcement","text":"Compliant Kubernetes integrates with the Open Policy Agent (OPA), which helps enforce policies automatically. Such policies can prevent e.g. the use of default passwords when connecting to databases, or configuration errors such as deploying a non-vetted Pod to production. Set up a policy that makes sense for your application and watch as OPA immediately stops violations to these policies to occur. It catches API requests to the Kubernetes API before they can touch resources in the cluster.","title":"Policies and automatic enforcement"},{"location":"benefits/#network-isolationsegregation","text":"Set up standard Kubernetes Network Policies such that: only the front end is publicly exposed; the front end can only initiate connections to the back end; the back end can only be connected to from the front end (no other components running in other namespaces); the back end only gets to initiate connections to the database; nothing besides the back end application gets to connect to the database; and the database may never initiate connections to anything on its own. Doing this shows that not only do you have network isolation/segregation up and running, but also, you have significantly reduced your attack surface. Should code get exploited in either component, it will still be limited in what damage it can do to the overall system.","title":"Network isolation/segregation"},{"location":"benefits/#automatic-certificate-management","text":"Of course your publicly exposed front end should support (only?) encrypted traffic. Set up the cert-manager to give your exposed service a certificate issued by Let's Encrypt.","title":"Automatic certificate management"},{"location":"compliance/","text":"Compliance Basics Compliance will vary widely depending on: Jurisdiction (e.g., US vs. EU); Industry regulation (e.g., MedTech vs. FinTech); Company policies (e.g., log retention based on cost-risk analysis). The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden. Click on the revelant blue text to find out more: Compliance: The Societal Perspective Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example: Any bank operating in Sweden is regulated by the Swedish Financial Authority (Finansinspektionen) and has to comply with FFFS. Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR. Any organization handling patient data needs to comply with HIPAA in the US or Patientdatalagen (PDL) in Sweden. Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose is license to operate. Compliance: The Engineering Perspective Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard , such as ISO/IEC 27001, SOC 2 or PCI DSS. Such information security standards list a set of controls , i.e., \"points\" in the organization where a process and a check needs to be put in place. The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team. In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. It reduces compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant. As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.","title":"Compliance Basics"},{"location":"compliance/#compliance-basics","text":"Compliance will vary widely depending on: Jurisdiction (e.g., US vs. EU); Industry regulation (e.g., MedTech vs. FinTech); Company policies (e.g., log retention based on cost-risk analysis). The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden. Click on the revelant blue text to find out more:","title":"Compliance Basics"},{"location":"compliance/#compliance-the-societal-perspective","text":"Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example: Any bank operating in Sweden is regulated by the Swedish Financial Authority (Finansinspektionen) and has to comply with FFFS. Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR. Any organization handling patient data needs to comply with HIPAA in the US or Patientdatalagen (PDL) in Sweden. Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose is license to operate.","title":"Compliance: The Societal Perspective"},{"location":"compliance/#compliance-the-engineering-perspective","text":"Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard , such as ISO/IEC 27001, SOC 2 or PCI DSS. Such information security standards list a set of controls , i.e., \"points\" in the organization where a process and a check needs to be put in place. The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team. In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. It reduces compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant. As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.","title":"Compliance: The Engineering Perspective"},{"location":"concepts/","text":"Concepts This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts . Control : \"Points\" in an organization that need a clear policy in order to comply with regulation. Regulation : Law or contractual requirements that an organization is required to follow to be allowed to operate. Operator : A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation. Service cluster : Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations. Workload cluster : Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality. User : A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.","title":"Core Concepts"},{"location":"concepts/#concepts","text":"This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts . Control : \"Points\" in an organization that need a clear policy in order to comply with regulation. Regulation : Law or contractual requirements that an organization is required to follow to be allowed to operate. Operator : A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation. Service cluster : Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations. Workload cluster : Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality. User : A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.","title":"Concepts"},{"location":"faq/","text":"Why can't I kubectl run ? To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\". Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root . See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label run: blah . Note This is just an example, not a good idea! You should limit the policy to whatever your application really needs. kubectl apply -f - <<EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: blah spec: podSelector: matchLabels: run: blah policyTypes: - Ingress - Egress ingress: # Allow all incoming traffic - {} egress: # Allow all outgoing traffic - {} EOF Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label run: <name-of-pod> when you create a Pod with kubectl run <name-of-pod> . Here is an example command (please replace the $MY_HARBOR_IMAGE ): kubectl run blah --rm -ti --image = $MY_HARBOR_IMAGE If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command: --overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }'","title":"FAQ"},{"location":"faq/#why-cant-i-kubectl-run","text":"To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\". Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root . See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label run: blah . Note This is just an example, not a good idea! You should limit the policy to whatever your application really needs. kubectl apply -f - <<EOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: blah spec: podSelector: matchLabels: run: blah policyTypes: - Ingress - Egress ingress: # Allow all incoming traffic - {} egress: # Allow all outgoing traffic - {} EOF Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label run: <name-of-pod> when you create a Pod with kubectl run <name-of-pod> . Here is an example command (please replace the $MY_HARBOR_IMAGE ): kubectl run blah --rm -ti --image = $MY_HARBOR_IMAGE If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command: --overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }'","title":"Why can't I kubectl run?"},{"location":"getting-started/","text":"Getting Started As a user, you will need: docker helm kubectl oidc-login ; we suggest installing it via krew The easier is to request a demo environment from a managed Compliant Kubernetes provider . You should receive: URLs for Compliant Kubernetes UI components, such as the dashboard, container registry, logs, etc. A kubeconfig file for configuring kubectl access to the cluster. (Optionally) Static username and password. Normally, you should log in via a username and a password of your organizations identity provider. If you want to setup your own Compliant Kubernetes installation, head to the Operator Manual .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"As a user, you will need: docker helm kubectl oidc-login ; we suggest installing it via krew The easier is to request a demo environment from a managed Compliant Kubernetes provider . You should receive: URLs for Compliant Kubernetes UI components, such as the dashboard, container registry, logs, etc. A kubeconfig file for configuring kubectl access to the cluster. (Optionally) Static username and password. Normally, you should log in via a username and a password of your organizations identity provider. If you want to setup your own Compliant Kubernetes installation, head to the Operator Manual .","title":"Getting Started"},{"location":"roadmap/","text":"Roadmap Light Compliant Kubernetes Renderings Some users have requested a Compliant Kubernetes workload cluster, but with external container registry or logging solution. Compliant Kubernetes needs to be revised to facilitate this. Multi-Region High-Availability Some business continuity policies may require redundancy across regions. It should be able to run Compliant Kubernetes across regions of a cloud provider. More generic Multi-Tenancy Right now, one service cluster can support multiple workload clusters, as long as they all use the same identity provider and reside in the same cloud provider. In the future, we want to relax these requirements for more flexible multi-tenancy scenarios. Support to run on top of multiple Kubernetes distributions, included managed services. Compliant Kubernetes can be configured on top of Kubernetes clusters created with kubespray or similar tools. It could be possible to run Compliant Kubernetes on top of managed Kubernetes services such as GKE, as well as distributions such as OpenShift. Out of the box service mesh support Compliant Kubernetes can be used with service meshes such as Istio and LinkerD, but does not come with a preconfigured service mesh installer as of now. Compliant Managed Services on top of Compliant Kubernetes With improved support for stateful applications and operators in Kubernetes, it is now possible to offer managed services on top of Kubernetes. Said services should reduce compliance burden, hence building them on top of Compliant Kubernetes is natural. The following managed services are envisioned for cloud providers who use Compliant Kubernetes: Managed Container Registry (e.g., Harbor); Managed Database (e.g., MariaDB, MySQL, PostgreSQL); Managed Message Queues (e.g., NATS, Kafka); Managed Caches (e.g., Redis); Managed Logging (e.g., Elasticsearch). Non-Goals CI/CD Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not be opinionated and prescribe a certain CI/CD technology.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#light-compliant-kubernetes-renderings","text":"Some users have requested a Compliant Kubernetes workload cluster, but with external container registry or logging solution. Compliant Kubernetes needs to be revised to facilitate this.","title":"Light Compliant Kubernetes Renderings"},{"location":"roadmap/#multi-region-high-availability","text":"Some business continuity policies may require redundancy across regions. It should be able to run Compliant Kubernetes across regions of a cloud provider.","title":"Multi-Region High-Availability"},{"location":"roadmap/#more-generic-multi-tenancy","text":"Right now, one service cluster can support multiple workload clusters, as long as they all use the same identity provider and reside in the same cloud provider. In the future, we want to relax these requirements for more flexible multi-tenancy scenarios.","title":"More generic Multi-Tenancy"},{"location":"roadmap/#support-to-run-on-top-of-multiple-kubernetes-distributions-included-managed-services","text":"Compliant Kubernetes can be configured on top of Kubernetes clusters created with kubespray or similar tools. It could be possible to run Compliant Kubernetes on top of managed Kubernetes services such as GKE, as well as distributions such as OpenShift.","title":"Support to run on top of multiple Kubernetes distributions, included managed services."},{"location":"roadmap/#out-of-the-box-service-mesh-support","text":"Compliant Kubernetes can be used with service meshes such as Istio and LinkerD, but does not come with a preconfigured service mesh installer as of now.","title":"Out of the box service mesh support"},{"location":"roadmap/#compliant-managed-services-on-top-of-compliant-kubernetes","text":"With improved support for stateful applications and operators in Kubernetes, it is now possible to offer managed services on top of Kubernetes. Said services should reduce compliance burden, hence building them on top of Compliant Kubernetes is natural. The following managed services are envisioned for cloud providers who use Compliant Kubernetes: Managed Container Registry (e.g., Harbor); Managed Database (e.g., MariaDB, MySQL, PostgreSQL); Managed Message Queues (e.g., NATS, Kafka); Managed Caches (e.g., Redis); Managed Logging (e.g., Elasticsearch).","title":"Compliant Managed Services on top of Compliant Kubernetes"},{"location":"roadmap/#non-goals","text":"","title":"Non-Goals"},{"location":"roadmap/#cicd","text":"Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not be opinionated and prescribe a certain CI/CD technology.","title":"CI/CD"},{"location":"adr/","text":"Architectural Decision Log This log lists the architectural decisions for Compliant Kubernetes. ADR-0000 - Use Markdown Architectural Decision Records ADR-0001 - Use Rook for Storage Orchestrator ADR-0002 - Use Kubespray for Cluster Life-cycle ADR-0003 - Push Metrics via InfluxDB ADR-0004 - Plan for Usage without Wrapper Scripts ADR-0005 - Use Individual SSH Keys ADR-0008 - Use HostNetwork or LoadBalancer for Ingress For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"Architectural Decision Log"},{"location":"adr/#architectural-decision-log","text":"This log lists the architectural decisions for Compliant Kubernetes. ADR-0000 - Use Markdown Architectural Decision Records ADR-0001 - Use Rook for Storage Orchestrator ADR-0002 - Use Kubespray for Cluster Life-cycle ADR-0003 - Push Metrics via InfluxDB ADR-0004 - Plan for Usage without Wrapper Scripts ADR-0005 - Use Individual SSH Keys ADR-0008 - Use HostNetwork or LoadBalancer for Ingress For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"Architectural Decision Log"},{"location":"adr/0000-use-markdown-architectural-decision-records/","text":"Use Markdown Architectural Decision Records Context and Problem Statement We want to record architectural decisions made in this project. Which format and structure should these records follow? Considered Options MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Formless \u2013 No conventions for file format and structure Decision Outcome Chosen option: \"MADR 2.1.2\", because We need to start somewhere, and it's better to have some format than no format. MADR seems to be good enough for our current needs.","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0000-use-markdown-architectural-decision-records/#use-markdown-architectural-decision-records","text":"","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","text":"We want to record architectural decisions made in this project. Which format and structure should these records follow?","title":"Context and Problem Statement"},{"location":"adr/0000-use-markdown-architectural-decision-records/#considered-options","text":"MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Formless \u2013 No conventions for file format and structure","title":"Considered Options"},{"location":"adr/0000-use-markdown-architectural-decision-records/#decision-outcome","text":"Chosen option: \"MADR 2.1.2\", because We need to start somewhere, and it's better to have some format than no format. MADR seems to be good enough for our current needs.","title":"Decision Outcome"},{"location":"adr/0001-use-rook-storage-orchestrator/","text":"Use Rook for Storage Orchestrator Status: accepted Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv Date: 2020-11-16 Context and Problem Statement Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases? Decision Drivers Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters. Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call operators are not constantly woken up. Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.) Considered Options Rook GlusterFS Longhorn NFS Storage Provider Decision Outcome Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance. Positive Consequences We no longer need to worry about cloud provider without native storage. Negative Consequences We need to deprecate our NFS storage provider. Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it. Pros and Cons of the Options Longhorn Good, because it is a CNCF project. Good, because it is well integrated with Kubernetes. Bad, because it is not the most mature CNCF project in the storage class. Bad, because it was not easy to set up. GlusterFS Good, because it is battle-tested. Bad, because it is not as well integrated with Kubernetes as other projects. Bad, because it is not a CNCF project (driven by Red Hat). NFS Storage Provider Good, because we used it before and we have experience. Bad, because it is a non-redundant, snowflake, brittle solution.","title":"Use Rook for Storage Orchestrator"},{"location":"adr/0001-use-rook-storage-orchestrator/#use-rook-for-storage-orchestrator","text":"Status: accepted Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv Date: 2020-11-16","title":"Use Rook for Storage Orchestrator"},{"location":"adr/0001-use-rook-storage-orchestrator/#context-and-problem-statement","text":"Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?","title":"Context and Problem Statement"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-drivers","text":"Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters. Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call operators are not constantly woken up. Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)","title":"Decision Drivers"},{"location":"adr/0001-use-rook-storage-orchestrator/#considered-options","text":"Rook GlusterFS Longhorn NFS Storage Provider","title":"Considered Options"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-outcome","text":"Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.","title":"Decision Outcome"},{"location":"adr/0001-use-rook-storage-orchestrator/#positive-consequences","text":"We no longer need to worry about cloud provider without native storage.","title":"Positive Consequences"},{"location":"adr/0001-use-rook-storage-orchestrator/#negative-consequences","text":"We need to deprecate our NFS storage provider. Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.","title":"Negative Consequences"},{"location":"adr/0001-use-rook-storage-orchestrator/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/0001-use-rook-storage-orchestrator/#longhorn","text":"Good, because it is a CNCF project. Good, because it is well integrated with Kubernetes. Bad, because it is not the most mature CNCF project in the storage class. Bad, because it was not easy to set up.","title":"Longhorn"},{"location":"adr/0001-use-rook-storage-orchestrator/#glusterfs","text":"Good, because it is battle-tested. Bad, because it is not as well integrated with Kubernetes as other projects. Bad, because it is not a CNCF project (driven by Red Hat).","title":"GlusterFS"},{"location":"adr/0001-use-rook-storage-orchestrator/#nfs-storage-provider","text":"Good, because we used it before and we have experience. Bad, because it is a non-redundant, snowflake, brittle solution.","title":"NFS Storage Provider"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/","text":"Use Kubespray for Cluster Life-cycle Status: accepted Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember) Date: 2020-11-17 Context and Problem Statement Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house ck8s-cluster implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters. Decision Drivers We want to differentiate on top of vanilla Kubernetes cluster. We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible. We promise building on top of best-of-breeds open source projets. We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management. Considered Options Rancher kubeadm via in-house tools (ck8s-cluster) kubespray kops Decision Outcome We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters. Positive Consequences We learn how to use a widely-used tool for cluster lifecycle management. We support many cloud providers. We can differentiate on top of vanilla Kubernetes. Negative Consequences We need training on kubespray. We need to port our tooling and practices to kubespray. We need to port compliantkubernetes-apps to work on kubespray. Pros and Cons of the Options Rancher Good, because it provides cluster life-cycle management at scale. Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs. Bad, because it is not a community project, hence entails long-term licensing uncertainty. kubeadm via in-house tool (ck8s-cluster) Good, because we know it and we built it. Good, because it works well for current use-cases. Bad, because it entails a lot of effort to develop and maintain. Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions. kops Good, because it integrates well with the underlying cloud provider (e.g., AWS). Bad, because it supports fewer cloud providers than kubespray. NOTE: In the future, we might want to support compliantkubernetes-apps on top of both kops and kubespray, but this does not seem to bring value just now.","title":"Use Kubespray for Cluster Life-cycle"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#use-kubespray-for-cluster-life-cycle","text":"Status: accepted Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember) Date: 2020-11-17","title":"Use Kubespray for Cluster Life-cycle"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#context-and-problem-statement","text":"Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house ck8s-cluster implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters.","title":"Context and Problem Statement"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-drivers","text":"We want to differentiate on top of vanilla Kubernetes cluster. We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible. We promise building on top of best-of-breeds open source projets. We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.","title":"Decision Drivers"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#considered-options","text":"Rancher kubeadm via in-house tools (ck8s-cluster) kubespray kops","title":"Considered Options"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-outcome","text":"We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters.","title":"Decision Outcome"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#positive-consequences","text":"We learn how to use a widely-used tool for cluster lifecycle management. We support many cloud providers. We can differentiate on top of vanilla Kubernetes.","title":"Positive Consequences"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#negative-consequences","text":"We need training on kubespray. We need to port our tooling and practices to kubespray. We need to port compliantkubernetes-apps to work on kubespray.","title":"Negative Consequences"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#rancher","text":"Good, because it provides cluster life-cycle management at scale. Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs. Bad, because it is not a community project, hence entails long-term licensing uncertainty.","title":"Rancher"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kubeadm-via-in-house-tool-ck8s-cluster","text":"Good, because we know it and we built it. Good, because it works well for current use-cases. Bad, because it entails a lot of effort to develop and maintain. Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions.","title":"kubeadm via in-house tool (ck8s-cluster)"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kops","text":"Good, because it integrates well with the underlying cloud provider (e.g., AWS). Bad, because it supports fewer cloud providers than kubespray. NOTE: In the future, we might want to support compliantkubernetes-apps on top of both kops and kubespray, but this does not seem to bring value just now.","title":"kops"},{"location":"adr/0003-push-metrics-via-influxdb/","text":"Push Metrics via InfluxDB Status: accepted Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik Date: 2020-11-19 Context and Problem Statement We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters: Dex, for authentication; Elastisearch, for pushing logs (append-only). Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster. Decision Drivers We want to support workload multi-tenancy. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster. Considered Options Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB. Migrate from InfluxDB to Thanos Migrate from InfluxDB to Cortex Decision Outcome We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible. Positive Consequences All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish. Negative Consequences Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the prometheus.$opsDomain DNS entry. The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints. The workload clusters will have to properly label their metrics. Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage. Pros and Cons of the Options Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future. However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.","title":"Push Metrics via InfluxDB"},{"location":"adr/0003-push-metrics-via-influxdb/#push-metrics-via-influxdb","text":"Status: accepted Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik Date: 2020-11-19","title":"Push Metrics via InfluxDB"},{"location":"adr/0003-push-metrics-via-influxdb/#context-and-problem-statement","text":"We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters: Dex, for authentication; Elastisearch, for pushing logs (append-only). Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster.","title":"Context and Problem Statement"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-drivers","text":"We want to support workload multi-tenancy. We want to untangle the life-cycle of the service cluster and workload cluster. The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.","title":"Decision Drivers"},{"location":"adr/0003-push-metrics-via-influxdb/#considered-options","text":"Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB. Migrate from InfluxDB to Thanos Migrate from InfluxDB to Cortex","title":"Considered Options"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-outcome","text":"We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible.","title":"Decision Outcome"},{"location":"adr/0003-push-metrics-via-influxdb/#positive-consequences","text":"All of *.$opsDomain can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup. Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy. The service cluster can be set up first, followed by one-or-more workload clusters. Workload clusters become more \"cattle\"-ish.","title":"Positive Consequences"},{"location":"adr/0003-push-metrics-via-influxdb/#negative-consequences","text":"Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the prometheus.$opsDomain DNS entry. The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints. The workload clusters will have to properly label their metrics. Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage.","title":"Negative Consequences"},{"location":"adr/0003-push-metrics-via-influxdb/#pros-and-cons-of-the-options","text":"Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future. However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.","title":"Pros and Cons of the Options"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/","text":"Plan for Usage without Wrapper Scripts Status: accepted Deciders: Architecture Meeting Date: 2020-11-24 Context and Problem Statement We frequently write wrapper scripts. They bring the following value: They bind together several tools and make them work together as a whole, e.g., sops and kubectl . They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files. They enforce best practices, e.g., encrypt secrets consumed or produced by tools. Unfortunately, wrapper scripts can also bring disadvantages: They make usages that are deviating from the \"good way\" difficult. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked. They add overhead when adding new features or supporting new use-cases. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions . Decision Drivers We want to make operations simple, predictable, resilient to human error and scalable. We want to have some predictability in how an environment is set up. We want to make Compliant Kubernetes flexible and agile. Considered Options On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented. On the other extreme, we completely \"ban\" wrapper scripts. Decision Outcome We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value. This decision applies for new wrapper scripts. We will not rework old wrapper scripts. Positive Consequences The operations team can encode standard operating procedures and scale ways of working. Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort. Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability. Negative Consequences There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.","title":"Plan for Usage without Wrapper Scripts"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#plan-for-usage-without-wrapper-scripts","text":"Status: accepted Deciders: Architecture Meeting Date: 2020-11-24","title":"Plan for Usage without Wrapper Scripts"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#context-and-problem-statement","text":"We frequently write wrapper scripts. They bring the following value: They bind together several tools and make them work together as a whole, e.g., sops and kubectl . They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files. They enforce best practices, e.g., encrypt secrets consumed or produced by tools. Unfortunately, wrapper scripts can also bring disadvantages: They make usages that are deviating from the \"good way\" difficult. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked. They add overhead when adding new features or supporting new use-cases. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions .","title":"Context and Problem Statement"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-drivers","text":"We want to make operations simple, predictable, resilient to human error and scalable. We want to have some predictability in how an environment is set up. We want to make Compliant Kubernetes flexible and agile.","title":"Decision Drivers"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#considered-options","text":"On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented. On the other extreme, we completely \"ban\" wrapper scripts.","title":"Considered Options"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-outcome","text":"We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value. This decision applies for new wrapper scripts. We will not rework old wrapper scripts.","title":"Decision Outcome"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#positive-consequences","text":"The operations team can encode standard operating procedures and scale ways of working. Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort. Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.","title":"Positive Consequences"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#negative-consequences","text":"There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.","title":"Negative Consequences"},{"location":"adr/0005-use-individual-ssh-keys/","text":"Use Individual SSH Keys Status: accepted Deciders: Cristian, Fredrik, Olle, Johan Date: 2021-01-28 Technical Story: Do not fiddle with the SSH key Create a process of how we should move to use personal SSH keys Context and Problem Statement Currently, we create per-cluster SSH key pairs, which are shared among operators. This is problematic from an information security perspective for a few reasons: It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes master. It makes credential management challenging, e.g., when onboarding/offboarding operators. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all operators. It encourages storing the SSH key pair without password protection. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey. It violates the Principle of Least Astonishment. Decision Drivers We need to stick to information security best-practices. Considered Options Inject SSH keys via cloud-init. Manage SSH keys via an Ansible role. Decision Outcome We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The public SSH keys of all operators will be put in a file in ck8s-ops , one key per line. The comment of the key needs to clearly identify the owner. The compliantkubernetes-kubespray project will make it easy to configure SSH keys. Operator logs (be it stand-alone documents, git or GitOps-like repositories) will clearly list the SSH keys and identities of the operators configured for each environment. Usually, the operators of each environment will be the members of the Elastisys Ops team. However, there could be exceptions: For development, PoC or demo environments, we might want to give access to more people; If the Elastisys Ops team acts as second-line operations, we might need to give access to first-line operators too. Bootstrapping The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows. For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init: AWS Exoscale GCP OpenStack The operator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other operators. BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal operator. Links ansible.posix.authorized_key Ansible Module","title":"Use Individual SSH Keys"},{"location":"adr/0005-use-individual-ssh-keys/#use-individual-ssh-keys","text":"Status: accepted Deciders: Cristian, Fredrik, Olle, Johan Date: 2021-01-28 Technical Story: Do not fiddle with the SSH key Create a process of how we should move to use personal SSH keys","title":"Use Individual SSH Keys"},{"location":"adr/0005-use-individual-ssh-keys/#context-and-problem-statement","text":"Currently, we create per-cluster SSH key pairs, which are shared among operators. This is problematic from an information security perspective for a few reasons: It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes master. It makes credential management challenging, e.g., when onboarding/offboarding operators. It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all operators. It encourages storing the SSH key pair without password protection. It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey. It violates the Principle of Least Astonishment.","title":"Context and Problem Statement"},{"location":"adr/0005-use-individual-ssh-keys/#decision-drivers","text":"We need to stick to information security best-practices.","title":"Decision Drivers"},{"location":"adr/0005-use-individual-ssh-keys/#considered-options","text":"Inject SSH keys via cloud-init. Manage SSH keys via an Ansible role.","title":"Considered Options"},{"location":"adr/0005-use-individual-ssh-keys/#decision-outcome","text":"We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The public SSH keys of all operators will be put in a file in ck8s-ops , one key per line. The comment of the key needs to clearly identify the owner. The compliantkubernetes-kubespray project will make it easy to configure SSH keys. Operator logs (be it stand-alone documents, git or GitOps-like repositories) will clearly list the SSH keys and identities of the operators configured for each environment. Usually, the operators of each environment will be the members of the Elastisys Ops team. However, there could be exceptions: For development, PoC or demo environments, we might want to give access to more people; If the Elastisys Ops team acts as second-line operations, we might need to give access to first-line operators too.","title":"Decision Outcome"},{"location":"adr/0005-use-individual-ssh-keys/#bootstrapping","text":"The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows. For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init: AWS Exoscale GCP OpenStack The operator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other operators. BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal operator.","title":"Bootstrapping"},{"location":"adr/0005-use-individual-ssh-keys/#links","text":"ansible.posix.authorized_key Ansible Module","title":"Links"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/","text":"Use HostNetwork or LoadBalancer for Ingress Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Technical Story: Ingress configuration Context and Problem Statement Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager . As of February 2021, Compliant Kubernetes comes by default with nginx-ingress , but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller? Decision Drivers We want to obey the Principle of Least Astonishment . We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer . Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. We want to keep things simple. Considered Options Via the host network , i.e., some workers expose the Ingress controller on their port 80 and 443. Over a NodePort service , i.e., kube-proxy exposes the Ingress controller on a port between 30000-32767 on each worker. As a Service Type LoadBalancer , i.e., above plus Kubernetes provisions a load balancer via Service controller . Decision Outcome Chosen options: Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes: Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead. Exoscale currently falls in this category, due to its Kubernetes integration being rather recent. SafeSpring falls in this category, since it is missing load balancers. If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud. Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.: *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing. Positive Consequences We make the best of each cloud provider. Obeys principle of least astonishment. We do not add a load balancer \"just because\". Negative Consequences Complexity is a bit increased, however, this feels like essential complexity. Links Cloud Controller Manager Ingress Nginx: Bare Metal Considerations","title":"Use HostNetwork or LoadBalancer for Ingress"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#use-hostnetwork-or-loadbalancer-for-ingress","text":"Status: accepted Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor Date: 2021-02-09 Technical Story: Ingress configuration","title":"Use HostNetwork or LoadBalancer for Ingress"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#context-and-problem-statement","text":"Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager . As of February 2021, Compliant Kubernetes comes by default with nginx-ingress , but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?","title":"Context and Problem Statement"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-drivers","text":"We want to obey the Principle of Least Astonishment . We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer . Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer. We want to keep things simple.","title":"Decision Drivers"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#considered-options","text":"Via the host network , i.e., some workers expose the Ingress controller on their port 80 and 443. Over a NodePort service , i.e., kube-proxy exposes the Ingress controller on a port between 30000-32767 on each worker. As a Service Type LoadBalancer , i.e., above plus Kubernetes provisions a load balancer via Service controller .","title":"Considered Options"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-outcome","text":"Chosen options: Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes: Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead. Exoscale currently falls in this category, due to its Kubernetes integration being rather recent. SafeSpring falls in this category, since it is missing load balancers. If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud. Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.: *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing.","title":"Decision Outcome"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#positive-consequences","text":"We make the best of each cloud provider. Obeys principle of least astonishment. We do not add a load balancer \"just because\".","title":"Positive Consequences"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#negative-consequences","text":"Complexity is a bit increased, however, this feels like essential complexity.","title":"Negative Consequences"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#links","text":"Cloud Controller Manager Ingress Nginx: Bare Metal Considerations","title":"Links"},{"location":"adr/template/","text":"[short title of solved problem and solution] Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options [option 1] [option 2] [option 3] \u2026 Decision Outcome Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options [option 1] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr/template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr/template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr/template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr/template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr/template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr/template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr/template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr/template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr/template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr/template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"ciso-guide/","text":"CISO Guide Overview This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform. The CISO can be described via the following user stories: As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies. As an information security officer, I want to quickly identify compliance violation and covert them into actionable tasks for developers. The CISO only needs: a modern browser (recent versions of Chrome, Firefox or Edge will do); the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com); credentials for the Compliant Kubernetes cluster. If in doubt, contact the Compliant Kubernetes operator.","title":"Overview"},{"location":"ciso-guide/#ciso-guide-overview","text":"This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform. The CISO can be described via the following user stories: As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies. As an information security officer, I want to quickly identify compliance violation and covert them into actionable tasks for developers. The CISO only needs: a modern browser (recent versions of Chrome, Firefox or Edge will do); the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com); credentials for the Compliant Kubernetes cluster. If in doubt, contact the Compliant Kubernetes operator.","title":"CISO Guide Overview"},{"location":"ciso-guide/backup/","text":"Backup Dashboard Relevant Regulations GDPR Article 32 : In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss , alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (A) Data backup plan (Required). Establish and implement procedures to create and maintain retrievable exact copies of electronic protected health information. (B) Disaster recovery plan (Required). Establish (and implement as needed) procedures to restore any loss of data. Mapping to ISO 27001 Controls A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Compliant Kubernetes Backup Dashboard The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met. Handling Non-Compliance In case there is a violation of backup policies: Ask the operator to check the status of the backup jobs . Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations .","title":"Backup"},{"location":"ciso-guide/backup/#backup-dashboard","text":"","title":"Backup Dashboard"},{"location":"ciso-guide/backup/#relevant-regulations","text":"GDPR Article 32 : In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss , alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (A) Data backup plan (Required). Establish and implement procedures to create and maintain retrievable exact copies of electronic protected health information. (B) Disaster recovery plan (Required). Establish (and implement as needed) procedures to restore any loss of data.","title":"Relevant Regulations"},{"location":"ciso-guide/backup/#mapping-to-iso-27001-controls","text":"A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/backup/#compliant-kubernetes-backup-dashboard","text":"The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met.","title":"Compliant Kubernetes Backup Dashboard"},{"location":"ciso-guide/backup/#handling-non-compliance","text":"In case there is a violation of backup policies: Ask the operator to check the status of the backup jobs . Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations .","title":"Handling Non-Compliance"},{"location":"ciso-guide/cryptography/","text":"Cryptography Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (ii) Encryption (Addressable). Implement a mechanism to encrypt electronic protected health information whenever deemed appropriate. Mapping to ISO 27001 Controls A.10 Cryptography Compliant Kubernetes Cryptography Dashboard The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically review certificates before expiry. Handling Non-Compliance In case there is a violation of cryptography policies: If a certificate is expired and was not renewed, ask the operator to check the status of cert-manager and ingress-controller component. If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations .","title":"Cryptography"},{"location":"ciso-guide/cryptography/#cryptography-dashboard","text":"","title":"Cryptography Dashboard"},{"location":"ciso-guide/cryptography/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (ii) Encryption (Addressable). Implement a mechanism to encrypt electronic protected health information whenever deemed appropriate.","title":"Relevant Regulations"},{"location":"ciso-guide/cryptography/#mapping-to-iso-27001-controls","text":"A.10 Cryptography","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/cryptography/#compliant-kubernetes-cryptography-dashboard","text":"The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically review certificates before expiry.","title":"Compliant Kubernetes Cryptography Dashboard"},{"location":"ciso-guide/cryptography/#handling-non-compliance","text":"In case there is a violation of cryptography policies: If a certificate is expired and was not renewed, ask the operator to check the status of cert-manager and ingress-controller component. If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations .","title":"Handling Non-Compliance"},{"location":"ciso-guide/intrusion-detection/","text":"Intrusion Detection Dashboard Relevant Regulations GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (B) Protection from malicious software (Addressable). Procedures for guarding against, detecting, and reporting malicious software. [highlights added] Mapping to ISO 27001 Controls A.12.2.1 Controls Against Malware A.12.6.1 Management of Technical Vulnerabilities A.16.1.7 Collection of Evidence Compliant Kubernetes Intrusion Detection Dashboard The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in /etc ) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily. Handling Non-Compliance Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.","title":"Intrusion Detection"},{"location":"ciso-guide/intrusion-detection/#intrusion-detection-dashboard","text":"","title":"Intrusion Detection Dashboard"},{"location":"ciso-guide/intrusion-detection/#relevant-regulations","text":"GDPR Article 32 : Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data; In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted , stored or otherwise processed. [highlights added] HIPAA Part 164\u2014SECURITY AND PRIVACY (B) Protection from malicious software (Addressable). Procedures for guarding against, detecting, and reporting malicious software. [highlights added]","title":"Relevant Regulations"},{"location":"ciso-guide/intrusion-detection/#mapping-to-iso-27001-controls","text":"A.12.2.1 Controls Against Malware A.12.6.1 Management of Technical Vulnerabilities A.16.1.7 Collection of Evidence","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/intrusion-detection/#compliant-kubernetes-intrusion-detection-dashboard","text":"The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in /etc ) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily.","title":"Compliant Kubernetes Intrusion Detection Dashboard"},{"location":"ciso-guide/intrusion-detection/#handling-non-compliance","text":"Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker. In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.","title":"Handling Non-Compliance"},{"location":"ciso-guide/policy-as-code/","text":"Policy-as-Code Dashboard Relevant Regulations Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying. Mapping to ISO 27001 Controls A.18.2.2 Compliance with Security Policies & Standards A.18.2.3 Technical Compliance Review Compliant Kubernetes Policy-as-Code Dashboard Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational. Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes. Handling Non-Compliance If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes. If a policy is missing or too strict, contact the Compliant Kubernetes operators.","title":"Policy-as-Code"},{"location":"ciso-guide/policy-as-code/#policy-as-code-dashboard","text":"","title":"Policy-as-Code Dashboard"},{"location":"ciso-guide/policy-as-code/#relevant-regulations","text":"Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying.","title":"Relevant Regulations"},{"location":"ciso-guide/policy-as-code/#mapping-to-iso-27001-controls","text":"A.18.2.2 Compliance with Security Policies & Standards A.18.2.3 Technical Compliance Review","title":"Mapping to ISO 27001 Controls"},{"location":"ciso-guide/policy-as-code/#compliant-kubernetes-policy-as-code-dashboard","text":"Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational. Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes.","title":"Compliant Kubernetes Policy-as-Code Dashboard"},{"location":"ciso-guide/policy-as-code/#handling-non-compliance","text":"If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes. If a policy is missing or too strict, contact the Compliant Kubernetes operators.","title":"Handling Non-Compliance"},{"location":"operator-manual/","text":"Operator Manual Overview This manual is for Compliant Kubernetes operators. Operators can be described via the following user stories: As an operator I want to create/destroy/upgrade a Compliant Kubernetes cluster. As an operator I want to re-configure a Compliant Kubernetes cluster. As an on-call operator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion. As an on-call operator I want to be alerted when the Compliant Kubernetes cluster is unhealthy. As an on-call operator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.","title":"Overview"},{"location":"operator-manual/#operator-manual-overview","text":"This manual is for Compliant Kubernetes operators. Operators can be described via the following user stories: As an operator I want to create/destroy/upgrade a Compliant Kubernetes cluster. As an operator I want to re-configure a Compliant Kubernetes cluster. As an on-call operator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion. As an on-call operator I want to be alerted when the Compliant Kubernetes cluster is unhealthy. As an on-call operator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.","title":"Operator Manual Overview"},{"location":"operator-manual/aws/","text":"Compliant Kubernetes Deployment on AWS This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts: Deploying at least two vanilla Kubernetes clusters Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools . Setup Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS = \"testwc0\" BASE_DOMAIN = \"example.com\" Deploying vanilla Kubernetes clusters We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray Infrastructure Setup using Terraform Expose AWS credentials to Terraform. We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system: export TF_VAR_AWS_ACCESS_KEY_ID = \"www\" export TF_VAR_AWS_SECRET_ACCESS_KEY = \"xxx\" export TF_VAR_AWS_SSH_KEY_NAME = \"yyy\" export TF_VAR_AWS_DEFAULT_REGION = \"zzz\" Tip We suggest generating the SSH key locally, then importing it to AWS. Customize your infrastructure. Create a configuration for the service cluster and the workload cluster: pushd kubespray for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do cat contrib/terraform/aws/terraform.tfvars \\ | sed \\ -e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\" $CLUSTER \\\"@\" \\ -e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts- $CLUSTER \\\"@\" \\ -e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\ > inventory/terraform- $CLUSTER .tfvars done popd Review and, if needed, adjust the files in kubespray/inventory/ . Initialize and Apply Terraform. pushd kubespray/contrib/terraform/aws terraform init for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do terraform apply \\ -var-file = ../../../inventory/terraform- $CLUSTER .tfvars \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Important The Terraform state is stored in kubespray/inventory/tfstate-* . It is precious. Consider backing it up or using Terraform Cloud . Check that the Ansible inventory was properly generated. ls -l kubespray/inventory/hosts-* You may also want to check the AWS Console if the infrastructure was created correctly: Deploying vanilla Kubernetes clusters using Kubespray. With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder. Init the Kubespray config in your config path. export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_PGP_FP = <your GPG key ID> # retrieve with gpg --list-secret-keys for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS; do ./bin/ck8s-kubespray init $CLUSTER default ~/.ssh/id_rsa.pub # This parts needs refactoring, in the meanwhile: sed -e 's@^---$@@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e 's@^---$@@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e 's@^etcd_kubeadm_enabled:.*@#etcd_kubeadm_enabled: false@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml echo 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e 's@.*[^_]cloud_provider:.*@cloud_provider: aws@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e \"s@.*kube_oidc_auth:.*@kube_oidc_auth: true@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_url:.*@kube_oidc_url: https://dex.$BASE_DOMAIN@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_client_id:.*@kube_oidc_client_id: kubelogin@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_username_claim:.*@kube_oidc_username_claim: email@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml done Important The key in ~/.ssh/id_rsa.pub must match the key referenced in TF_VAR_AWS_SSH_KEY_NAME above. Copy the inventories generated by Terraform above in the right place. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do cp kubespray/inventory/hosts- $CLUSTER $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done Run kubespray to deploy the Kubernetes clusters. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 20 minutes. Correct the Kubernetes API IP addresses. Find the DNS names of the load balancers fronting the API servers: grep apiserver_loadbalancer $CK8S_CONFIG_PATH /*-config/inventory.ini Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the URL of the load balancer from inventory files shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do sops $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows: for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done Deploying Compliant Kubernetes Apps Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. If you haven't done so already, clone the Compliant Kubernetes apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration. export CK8S_ENVIRONMENT_NAME = aws #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_CLOUD_PROVIDER = aws export CK8S_PGP_FP = <your GPG key ID> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the $CK8S_CONFIG_PATH directory. ls -l $CK8S_CONFIG_PATH Configure the apps. Edit the configuration files sc-config.yaml , wc-config.yaml and secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim $CK8S_CONFIG_PATH /sc-config.yaml vim $CK8S_CONFIG_PATH /wc-config.yaml sops $CK8S_CONFIG_PATH /secrets.yaml The following are the minimum change you should perform: # sc-config.yaml and wc-config.yaml global: baseDomain: \"set-me\" # set to $BASE_DOMAIN opsDomain: \"set-me\" # set to ops.$BASE_DOMAIN issuer: letsencrypt-prod objectStorage: type: \"s3\" s3: region: \"set-me\" # Region for S3 buckets, e.g, eu-central-1 regionAddress: \"set-me\" # Region address, e.g, s3.eu-central-1.amazonaws.com regionEndpoint: \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com fluentd: forwarder: useRegionEndpoint: \"set-me\" # set it to either true or false issuers: letsencrypt: email: \"set-me\" # set this to an email to receive LetsEncrypt notifications # secrets.yaml objectStorage: s3: accessKey: \"set-me\" #put your s3 accesskey secretKey: \"set-me\" #put your s3 secretKey Create placeholder DNS entries. To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53): echo \" *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 \" NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder. Installing Compliant Kubernetes apps. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in $WORKLOAD_CLUSTERS; do ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done NOTE: Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. Setup required DNS entries. You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster : SC_INGRESS_LB_HOSTNAME=$(sops exec-file $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}') SC_INGRESS_LB_IP=$(dig +short $SC_INGRESS_LB_HOSTNAME | head -1) echo $SC_INGRESS_LB_IP Then, import the following zone in AWS Route53: echo \"\"\" *.ops.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP dex.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP grafana.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP harbor.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP kibana.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP \"\"\" Testing: After completing the installation step you can test if the apps are properly installed and ready using the commands below. ./bin/ck8s test sc for CLUSTER in $WORKLOAD_CLUSTERS ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc done Done. Navigate to grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown for CLUSTER in $WORKLOAD_CLUSTERS $SERVICE_CLUSTER ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} delete --all-namespaces --all ingress,service,deployment,statefulset,daemonset,cronjob,job,pod,sa,secret,configmap' done # Feel free to skips this step, but remember to remove EBS volumes manually # from the AWS Console, after Terraform teardown. for CLUSTER in $WORKLOAD_CLUSTERS $SERVICE_CLUSTER ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} delete --all-namespaces --all pvc,pv' done cd ../compliantkubernetes-kubespray pushd kubespray/contrib/terraform/aws for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do terraform destroy \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Further Reading Compliant Kubernetes apps repo Configurations option","title":"On AWS"},{"location":"operator-manual/aws/#compliant-kubernetes-deployment-on-aws","text":"This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts: Deploying at least two vanilla Kubernetes clusters Deploying Compliant Kubernetes apps Before starting, make sure you have all necessary tools .","title":"Compliant Kubernetes Deployment on AWS"},{"location":"operator-manual/aws/#setup","text":"Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS = \"testwc0\" BASE_DOMAIN = \"example.com\"","title":"Setup"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters","text":"We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray","title":"Deploying vanilla Kubernetes clusters"},{"location":"operator-manual/aws/#infrastructure-setup-using-terraform","text":"Expose AWS credentials to Terraform. We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system: export TF_VAR_AWS_ACCESS_KEY_ID = \"www\" export TF_VAR_AWS_SECRET_ACCESS_KEY = \"xxx\" export TF_VAR_AWS_SSH_KEY_NAME = \"yyy\" export TF_VAR_AWS_DEFAULT_REGION = \"zzz\" Tip We suggest generating the SSH key locally, then importing it to AWS. Customize your infrastructure. Create a configuration for the service cluster and the workload cluster: pushd kubespray for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do cat contrib/terraform/aws/terraform.tfvars \\ | sed \\ -e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\" $CLUSTER \\\"@\" \\ -e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts- $CLUSTER \\\"@\" \\ -e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\ > inventory/terraform- $CLUSTER .tfvars done popd Review and, if needed, adjust the files in kubespray/inventory/ . Initialize and Apply Terraform. pushd kubespray/contrib/terraform/aws terraform init for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do terraform apply \\ -var-file = ../../../inventory/terraform- $CLUSTER .tfvars \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd Important The Terraform state is stored in kubespray/inventory/tfstate-* . It is precious. Consider backing it up or using Terraform Cloud . Check that the Ansible inventory was properly generated. ls -l kubespray/inventory/hosts-* You may also want to check the AWS Console if the infrastructure was created correctly:","title":"Infrastructure Setup using Terraform"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters-using-kubespray","text":"With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder. Init the Kubespray config in your config path. export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_PGP_FP = <your GPG key ID> # retrieve with gpg --list-secret-keys for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS; do ./bin/ck8s-kubespray init $CLUSTER default ~/.ssh/id_rsa.pub # This parts needs refactoring, in the meanwhile: sed -e 's@^---$@@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e 's@^---$@@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e 's@^etcd_kubeadm_enabled:.*@#etcd_kubeadm_enabled: false@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml echo 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e 's@.*[^_]cloud_provider:.*@cloud_provider: aws@' -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/all/all.yml sed -e \"s@.*kube_oidc_auth:.*@kube_oidc_auth: true@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_url:.*@kube_oidc_url: https://dex.$BASE_DOMAIN@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_client_id:.*@kube_oidc_client_id: kubelogin@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml sed -e \"s@.*kube_oidc_username_claim:.*@kube_oidc_username_claim: email@\" -i $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s-cluster/k8s-cluster.yml done Important The key in ~/.ssh/id_rsa.pub must match the key referenced in TF_VAR_AWS_SSH_KEY_NAME above. Copy the inventories generated by Terraform above in the right place. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do cp kubespray/inventory/hosts- $CLUSTER $CK8S_CONFIG_PATH / $CLUSTER -config/inventory.ini done Run kubespray to deploy the Kubernetes clusters. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do ./bin/ck8s-kubespray apply $CLUSTER --flush-cache done This may take up to 20 minutes. Correct the Kubernetes API IP addresses. Find the DNS names of the load balancers fronting the API servers: grep apiserver_loadbalancer $CK8S_CONFIG_PATH /*-config/inventory.ini Locate the encrypted kubeconfigs kube_config_*.yaml and edit them using sops. Copy the URL of the load balancer from inventory files shown above into kube_config_*.yaml . Do not overwrite the port. for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do sops $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows: for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done","title":"Deploying vanilla Kubernetes clusters using Kubespray."},{"location":"operator-manual/aws/#deploying-compliant-kubernetes-apps","text":"Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps. If you haven't done so already, clone the Compliant Kubernetes apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration. export CK8S_ENVIRONMENT_NAME = aws #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/aws export CK8S_CLOUD_PROVIDER = aws export CK8S_PGP_FP = <your GPG key ID> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the $CK8S_CONFIG_PATH directory. ls -l $CK8S_CONFIG_PATH Configure the apps. Edit the configuration files sc-config.yaml , wc-config.yaml and secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim $CK8S_CONFIG_PATH /sc-config.yaml vim $CK8S_CONFIG_PATH /wc-config.yaml sops $CK8S_CONFIG_PATH /secrets.yaml The following are the minimum change you should perform: # sc-config.yaml and wc-config.yaml global: baseDomain: \"set-me\" # set to $BASE_DOMAIN opsDomain: \"set-me\" # set to ops.$BASE_DOMAIN issuer: letsencrypt-prod objectStorage: type: \"s3\" s3: region: \"set-me\" # Region for S3 buckets, e.g, eu-central-1 regionAddress: \"set-me\" # Region address, e.g, s3.eu-central-1.amazonaws.com regionEndpoint: \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com fluentd: forwarder: useRegionEndpoint: \"set-me\" # set it to either true or false issuers: letsencrypt: email: \"set-me\" # set this to an email to receive LetsEncrypt notifications # secrets.yaml objectStorage: s3: accessKey: \"set-me\" #put your s3 accesskey secretKey: \"set-me\" #put your s3 secretKey Create placeholder DNS entries. To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53): echo \" *.$BASE_DOMAIN 60s A 203.0.113.123 *.ops.$BASE_DOMAIN 60s A 203.0.113.123 \" NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder. Installing Compliant Kubernetes apps. Start with the service cluster: ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ SERVICE_CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_sc.yaml ./bin/ck8s apply sc # Respond \"n\" if you get a WARN Then the workload clusters: for CLUSTER in $WORKLOAD_CLUSTERS; do ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml ./bin/ck8s apply wc # Respond \"n\" if you get a WARN done NOTE: Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes. Setup required DNS entries. You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster : SC_INGRESS_LB_HOSTNAME=$(sops exec-file $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}') SC_INGRESS_LB_IP=$(dig +short $SC_INGRESS_LB_HOSTNAME | head -1) echo $SC_INGRESS_LB_IP Then, import the following zone in AWS Route53: echo \"\"\" *.ops.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP dex.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP grafana.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP harbor.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP kibana.$BASE_DOMAIN 60s A $SC_INGRESS_LB_IP \"\"\" Testing: After completing the installation step you can test if the apps are properly installed and ready using the commands below. ./bin/ck8s test sc for CLUSTER in $WORKLOAD_CLUSTERS ; do ln -sf $CK8S_CONFIG_PATH /.state/kube_config_ ${ CLUSTER } .yaml $CK8S_CONFIG_PATH /.state/kube_config_wc.yaml ./bin/ck8s test wc done Done. Navigate to grafana.$BASE_DOMAIN , kibana.$BASE_DOMAIN , harbor.$BASE_DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Deploying Compliant Kubernetes Apps"},{"location":"operator-manual/aws/#teardown","text":"for CLUSTER in $WORKLOAD_CLUSTERS $SERVICE_CLUSTER ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} delete --all-namespaces --all ingress,service,deployment,statefulset,daemonset,cronjob,job,pod,sa,secret,configmap' done # Feel free to skips this step, but remember to remove EBS volumes manually # from the AWS Console, after Terraform teardown. for CLUSTER in $WORKLOAD_CLUSTERS $SERVICE_CLUSTER ; do sops exec-file $CK8S_CONFIG_PATH /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} delete --all-namespaces --all pvc,pv' done cd ../compliantkubernetes-kubespray pushd kubespray/contrib/terraform/aws for CLUSTER in $SERVICE_CLUSTER $WORKLOAD_CLUSTERS ; do terraform destroy \\ -auto-approve \\ -state = ../../../inventory/tfstate- $CLUSTER .tfstate done popd","title":"Teardown"},{"location":"operator-manual/aws/#further-reading","text":"Compliant Kubernetes apps repo Configurations option","title":"Further Reading"},{"location":"operator-manual/break-glass/","text":"Work in progress Please check back soon!","title":"Breaking the Glass"},{"location":"operator-manual/break-glass/#work-in-progress","text":"Please check back soon!","title":"Work in progress"},{"location":"operator-manual/disaster-recovery/","text":"Disaster Recovery This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the operator. Compliant Need Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are: A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity Elasticsearch Backup Elasticsearch is set up to to store backups in an S3 bucket. There is a CronJob called elasticsearch-backup in the cluster that is invoking the snapshot process in elasticsearch. To take a snapshot on-demand, execute ./bin/ck8s ops kubectl sc -n elastic-system create job --from=cronjob/elasticsearch-backup <name-of-job> Restore Set the following variables user - Eelasticsearch user with permissions to manage snapshots, usually snapshotter password - password for the above user es_url - url to Elasticsearch List snapshot repositories # Simple \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/repositories?v\" id type s3_exoscale_7.x s3 # Detailed \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/?pretty\" { \"s3_exoscale_7.x\" : { \"type\" : \"s3\" , \"settings\" : { \"bucket\" : \"es-backup\" , \"client\" : \"default\" } } } List available snapshots snapshot_repo = <name/id from previous step> # Simple \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/snapshots/ ${ snapshot_repo } ?v&s=id\" id status start_epoch start_time end_epoch end_time duration indices successful_shards failed_shards total_shards snapshot-20200929_093941z SUCCESS 1601372382 09 :39:42 1601372390 09 :39:50 8 .4s 6 6 0 6 snapshot-20200930_000008z SUCCESS 1601424008 00 :00:08 1601424035 00 :00:35 27 .4s 20 20 0 20 snapshot-20201001_000006z SUCCESS 1601510407 00 :00:07 1601510530 00 :02:10 2m 75 75 0 75 # Detailed list of all snapshots curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /_all?pretty\" # Detailed list of specific snapshot \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /snapshot-20201001_000006z?pretty\" { \"snapshots\" : [ { \"snapshot\" : \"snapshot-20201001_000006z\" , \"uuid\" : \"Fq0EusFYRV2nI9G9F1DX1A\" , \"version_id\" : 7080099 , \"version\" : \"7.8.0\" , \"indices\" : [ \"kubernetes-default-2020.09.30-000032\" , \"other-default-2020.09.30-000005\" , ..<redacted>.. \"kubeaudit-default-2020.09.30-000009\" ] , \"include_global_state\" : false, \"state\" : \"SUCCESS\" , \"start_time\" : \"2020-10-01T00:00:07.344Z\" , \"start_time_in_millis\" : 1601510407344 , \"end_time\" : \"2020-10-01T00:02:10.828Z\" , \"end_time_in_millis\" : 1601510530828 , \"duration_in_millis\" : 123484 , \"failures\" : [ ] , \"shards\" : { \"total\" : 75 , \"failed\" : 0 , \"successful\" : 75 } } ] } You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot snapshot_name = <Snapshot name from previous step> indices = \"<list of comma separated indices/index patterns>\" curl -k -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' ${ indices } '\" } ' Read the API to see all parameters and their explanations. Start new cluster from snapshot This process is very similar to the one described above, but there are a few extra steps to carry out. Before you install Elasticsearch you can preferably disable the initial index creation by setting configurer.createIndices: false in the values file for opendistro. This will make the restore process leaner. Install the Elasticsearch suite: ./bin/ck8s ops helmfile sc -l app = opendistro apply Wait for the the installation to complete. After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following indices variable indices = \"kubernetes-*,kubeaudit-*,other-*\" Note This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't: Register a new S3 snapshot repository to the old bucket as described here Use the newly registered snapshot repository in the restore process","title":"Disaster Recovery"},{"location":"operator-manual/disaster-recovery/#disaster-recovery","text":"This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the operator.","title":"Disaster Recovery"},{"location":"operator-manual/disaster-recovery/#compliant-need","text":"Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are: A.12.3.1 Information Backup A.17.1.1 Planning Information Security Continuity","title":"Compliant Need"},{"location":"operator-manual/disaster-recovery/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"operator-manual/disaster-recovery/#backup","text":"Elasticsearch is set up to to store backups in an S3 bucket. There is a CronJob called elasticsearch-backup in the cluster that is invoking the snapshot process in elasticsearch. To take a snapshot on-demand, execute ./bin/ck8s ops kubectl sc -n elastic-system create job --from=cronjob/elasticsearch-backup <name-of-job>","title":"Backup"},{"location":"operator-manual/disaster-recovery/#restore","text":"Set the following variables user - Eelasticsearch user with permissions to manage snapshots, usually snapshotter password - password for the above user es_url - url to Elasticsearch List snapshot repositories # Simple \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/repositories?v\" id type s3_exoscale_7.x s3 # Detailed \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/?pretty\" { \"s3_exoscale_7.x\" : { \"type\" : \"s3\" , \"settings\" : { \"bucket\" : \"es-backup\" , \"client\" : \"default\" } } } List available snapshots snapshot_repo = <name/id from previous step> # Simple \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_cat/snapshots/ ${ snapshot_repo } ?v&s=id\" id status start_epoch start_time end_epoch end_time duration indices successful_shards failed_shards total_shards snapshot-20200929_093941z SUCCESS 1601372382 09 :39:42 1601372390 09 :39:50 8 .4s 6 6 0 6 snapshot-20200930_000008z SUCCESS 1601424008 00 :00:08 1601424035 00 :00:35 27 .4s 20 20 0 20 snapshot-20201001_000006z SUCCESS 1601510407 00 :00:07 1601510530 00 :02:10 2m 75 75 0 75 # Detailed list of all snapshots curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /_all?pretty\" # Detailed list of specific snapshot \u276f curl -k -u \" ${ user } : ${ password } \" \" ${ es_url } /_snapshot/ ${ snapshot_repo } /snapshot-20201001_000006z?pretty\" { \"snapshots\" : [ { \"snapshot\" : \"snapshot-20201001_000006z\" , \"uuid\" : \"Fq0EusFYRV2nI9G9F1DX1A\" , \"version_id\" : 7080099 , \"version\" : \"7.8.0\" , \"indices\" : [ \"kubernetes-default-2020.09.30-000032\" , \"other-default-2020.09.30-000005\" , ..<redacted>.. \"kubeaudit-default-2020.09.30-000009\" ] , \"include_global_state\" : false, \"state\" : \"SUCCESS\" , \"start_time\" : \"2020-10-01T00:00:07.344Z\" , \"start_time_in_millis\" : 1601510407344 , \"end_time\" : \"2020-10-01T00:02:10.828Z\" , \"end_time_in_millis\" : 1601510530828 , \"duration_in_millis\" : 123484 , \"failures\" : [ ] , \"shards\" : { \"total\" : 75 , \"failed\" : 0 , \"successful\" : 75 } } ] } You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot snapshot_name = <Snapshot name from previous step> indices = \"<list of comma separated indices/index patterns>\" curl -k -u \" ${ user } : ${ password } \" -X POST \" ${ es_url } /_snapshot/ ${ snapshot_repo } / ${ snapshot_name } /_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"' ${ indices } '\" } ' Read the API to see all parameters and their explanations.","title":"Restore"},{"location":"operator-manual/disaster-recovery/#start-new-cluster-from-snapshot","text":"This process is very similar to the one described above, but there are a few extra steps to carry out. Before you install Elasticsearch you can preferably disable the initial index creation by setting configurer.createIndices: false in the values file for opendistro. This will make the restore process leaner. Install the Elasticsearch suite: ./bin/ck8s ops helmfile sc -l app = opendistro apply Wait for the the installation to complete. After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following indices variable indices = \"kubernetes-*,kubeaudit-*,other-*\" Note This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't: Register a new S3 snapshot repository to the old bucket as described here Use the newly registered snapshot repository in the restore process","title":"Start new cluster from snapshot"},{"location":"operator-manual/exoscale/","text":"Compliant Kubernetes Deployment on Exoscale This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it. Before starting, make sure you have all necessary tools . Setup Choose names for your service cluster and workload cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" ) Deploying Compliant Kubernetes clusters In this section the steps for infrastructure setup, Compliant Kubernetes deployment, and DNS record creation will be presented. Infrastructure Setup using Terraform Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray/kubespray NOTE: The repo will change when the Terraform code for Exoscale is pushed to Kuberspray upstream repo and may also affect the steps provided below. Expose Exoscale credentials to Terraform. For authentication create the file ~/.cloudstack.ini and put your Exoscale credentials in it. The file should look like something like this: [cloudstack] key = <API key> secret = <API secret> Customize your infrastructure. Create a configuration for the service and the workload clusters: for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp -r inventory/sample inventory/ $CLUSTER cp contrib/terraform/exoscale/default.tfvars inventory/ $CLUSTER / done Review and, if needed, adjust the files in inventory/$CLUSTER/default.tfvars , where $CLUSTER is the cluster name. Please use different value for the prefix field in /default.tfvars for the two clusters. Failing to do so will result in name conflict. Besides please also set a non-zero value for ceph_partition_size field, e.g., \"ceph_partition_size\": 50 , as it will be used by Rook storage service to provide local disk storage. To use whitelisting, replace \"0.0.0.0/0\" with the IP(s) you want to whitelist. Note that exoscale only allows one key pair of the same ssh-key. This means that for in least one of inventory/$CLUSTER/default.tfvars you need to specify another key in the \"ssh_pub_key\" field. Initialize and Apply Terraform. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do pushd inventory/ $CLUSTER terraform init ../../contrib/terraform/exoscale terraform apply \\ -var-file default.tfvars \\ -auto-approve \\ -state = tfstate- $CLUSTER .tfstate \\ ../../contrib/terraform/exoscale popd done NOTE: The Terraform state is stored in inventory/$CLUSTER/tfstate-$CLUSTER.tfstate , where $CLUSTER is the cluster name. It is precious. Consider backing it up our using Terraform Cloud . You should now have inventory file named inventory/$CLUSTER/inventory.ini for each cluster that you can use with kubespray. Deploying Compliant Kubernetes clusters using Kubespray With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the compliantkubernetes-kubespray root directory. cd .. 1. Init the Kubespray config in your config path. export DOMAIN = <your_domain> # DNS domain to expose the services inside the service cluster i.e. \"example.com\" export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s-kubespray init $CLUSTER default ~/.ssh/id_rsa # This parts needs refactoring, in the meanwhile: echo 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/all/all.yml done 2. Copy the inventory files generated by Terraform above in the right place. Please copy the two inventory files, {CK8S_CONFIG_PATH}/$CLUSTER-config/inventory.ini , generated by Terraform for the three clusters to ${CK8S_CONFIG_PATH}/$CLUSTER-config/ , where $CLUSTER the name of each cluster (i.e., testsc , testwc0 ). Run kubespray to deploy the Kubernetes clusters. mkdir -p ${ CK8S_CONFIG_PATH } /.state for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s-kubespray apply $CLUSTER done This may take up to 10 minutes for each cluster, 20 minutes in total. Correct the Kubernetes API IP addresses. Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer from inventory files ${CK8S_CONFIG_PATH}/*-config/inventory.ini and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows: for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done 6. Create the DNS Records You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup. To get the load-balancer IP, run the following command for each cluster and copy the value from the ingress_controller_lb_ip_address field. terraform output -state kubespray/ $CLUSTER /tfstate- $CLUSTER .tfstate Then point these domains to the service cluster using 'exoscale cli' as follows: exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n *.ops.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n grafana.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n harbor.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n kibana.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n dex.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n notary.harbor.<environment_name> Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record: exo dns add A $DOMAIN -a <workload_cluster_lb_ip> -n *.<environment_name> Deploy Rook To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. cd rook for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./deploy-rook.sh done Deploying Compliant Kubernetes Apps Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps and the Rook storage orchestration service. If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration. export CK8S_ENVIRONMENT_NAME = exoscale #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_CLOUD_PROVIDER = exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. Configure the apps. Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global: baseDomain: \"set-me\" # set to <enovironment_name>.$DOMAIN opsDomain: \"set-me\" # set to ops.<environment_name>.$DOMAIN issuer: letsencrypt-prod objectStorage: type: \"s3\" s3: region: \"set-me\" # Region for S3 buckets, e.g, west-1 regionAddress: \"set-me\" # Region address, e.g, s3.us-west-1.amazonaws.com regionEndpoint: \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses: default: rook-ceph-block nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) fluentd: forwarder: useRegionEndpoint: \"set-me\" # set it to either true or false elasticsearch: dataNode: storageClass: rook-ceph-block # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage: s3: accessKey: \"set-me\" #set to your s3 accesskey secretKey: \"set-me\" #set to your s3 secretKey Bootstrapping To deploy the Compliant Kubernetes apps, please go to the compliantkubernetes-apps repo root directory and run the following. export CK8S_CONFIG_PATH = ~/.ck8s/exoscale for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s bootstrap $CLUSTER done Installing Compliant Kubernetes apps. To deploy the Compliant Kubernetes apps, please go to the compliantkubernetes-apps repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s apps $CLUSTER done 7. Testing: After completing the installation step you can test if the apps are properly installed and ready using the commands below. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s test $CLUSTER done Done. Navigate to the endpoints, for example grafana.<environment_name>.$DOMAIN , kibana.<environment_name>.$DOMAIN , harbor.<environment_name>.$DOMAIN , etc. to discover Compliant Kubernetes's features. Teardown To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section). for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do terraform destroy \\ -var-file inventory/ $CLUSTER /default.tfvars \\ -auto-approve \\ -state = inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate \\ contrib/terraform/exoscale done # Remove dns records exo dns remove $DOMAIN *.ops.<environment_name> exo dns remove $DOMAIN grafana.<environment_name> exo dns remove $DOMAIN harbor.<environment_name> exo dns remove $DOMAIN kibana.<environment_name> exo dns remove $DOMAIN dex.<environment_name> exo dns remove $DOMAIN notary.harbor.<environment_name> exo dns remove $DOMAIN *.<environment_name> Further Reading Elastisys Compliant Kubernetes Kubespray Kubernetes on Exoscale with Terraform Compliant Kubernetes apps repo Configurations option","title":"On Exoscale"},{"location":"operator-manual/exoscale/#compliant-kubernetes-deployment-on-exoscale","text":"This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document: Infrastructure setup for two clusters: one service and one workload cluster Deploying Compliant Kubernetes on top of the two clusters. Creating DNS Records Deploying Rook Storage Orchestration Service Deploying Compliant Kubernetes apps The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it. Before starting, make sure you have all necessary tools .","title":"Compliant Kubernetes Deployment on Exoscale"},{"location":"operator-manual/exoscale/#setup","text":"Choose names for your service cluster and workload cluster: SERVICE_CLUSTER = \"testsc\" WORKLOAD_CLUSTERS =( \"testwc0\" )","title":"Setup"},{"location":"operator-manual/exoscale/#deploying-compliant-kubernetes-clusters","text":"In this section the steps for infrastructure setup, Compliant Kubernetes deployment, and DNS record creation will be presented.","title":"Deploying Compliant Kubernetes clusters"},{"location":"operator-manual/exoscale/#infrastructure-setup-using-terraform","text":"Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray/kubespray NOTE: The repo will change when the Terraform code for Exoscale is pushed to Kuberspray upstream repo and may also affect the steps provided below. Expose Exoscale credentials to Terraform. For authentication create the file ~/.cloudstack.ini and put your Exoscale credentials in it. The file should look like something like this: [cloudstack] key = <API key> secret = <API secret> Customize your infrastructure. Create a configuration for the service and the workload clusters: for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do cp -r inventory/sample inventory/ $CLUSTER cp contrib/terraform/exoscale/default.tfvars inventory/ $CLUSTER / done Review and, if needed, adjust the files in inventory/$CLUSTER/default.tfvars , where $CLUSTER is the cluster name. Please use different value for the prefix field in /default.tfvars for the two clusters. Failing to do so will result in name conflict. Besides please also set a non-zero value for ceph_partition_size field, e.g., \"ceph_partition_size\": 50 , as it will be used by Rook storage service to provide local disk storage. To use whitelisting, replace \"0.0.0.0/0\" with the IP(s) you want to whitelist. Note that exoscale only allows one key pair of the same ssh-key. This means that for in least one of inventory/$CLUSTER/default.tfvars you need to specify another key in the \"ssh_pub_key\" field. Initialize and Apply Terraform. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do pushd inventory/ $CLUSTER terraform init ../../contrib/terraform/exoscale terraform apply \\ -var-file default.tfvars \\ -auto-approve \\ -state = tfstate- $CLUSTER .tfstate \\ ../../contrib/terraform/exoscale popd done NOTE: The Terraform state is stored in inventory/$CLUSTER/tfstate-$CLUSTER.tfstate , where $CLUSTER is the cluster name. It is precious. Consider backing it up our using Terraform Cloud . You should now have inventory file named inventory/$CLUSTER/inventory.ini for each cluster that you can use with kubespray.","title":"Infrastructure Setup using Terraform"},{"location":"operator-manual/exoscale/#deploying-compliant-kubernetes-clusters-using-kubespray","text":"With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the compliantkubernetes-kubespray root directory. cd .. 1. Init the Kubespray config in your config path. export DOMAIN = <your_domain> # DNS domain to expose the services inside the service cluster i.e. \"example.com\" export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s-kubespray init $CLUSTER default ~/.ssh/id_rsa # This parts needs refactoring, in the meanwhile: echo 'ansible_user: ubuntu' >> $CK8S_CONFIG_PATH / $CLUSTER -config/group_vars/all/all.yml done 2. Copy the inventory files generated by Terraform above in the right place. Please copy the two inventory files, {CK8S_CONFIG_PATH}/$CLUSTER-config/inventory.ini , generated by Terraform for the three clusters to ${CK8S_CONFIG_PATH}/$CLUSTER-config/ , where $CLUSTER the name of each cluster (i.e., testsc , testwc0 ). Run kubespray to deploy the Kubernetes clusters. mkdir -p ${ CK8S_CONFIG_PATH } /.state for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s-kubespray apply $CLUSTER done This may take up to 10 minutes for each cluster, 20 minutes in total. Correct the Kubernetes API IP addresses. Locate the encrypted kubeconfigs in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml and edit them using sops. Copy the public IP address of the load balancer from inventory files ${CK8S_CONFIG_PATH}/*-config/inventory.ini and replace the private IP address for the server field in ${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml . for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml done Test access to the clusters as follows: for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops exec-file ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml \\ 'kubectl --kubeconfig {} get nodes' done 6. Create the DNS Records You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup. To get the load-balancer IP, run the following command for each cluster and copy the value from the ingress_controller_lb_ip_address field. terraform output -state kubespray/ $CLUSTER /tfstate- $CLUSTER .tfstate Then point these domains to the service cluster using 'exoscale cli' as follows: exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n *.ops.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n grafana.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n harbor.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n kibana.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n dex.<environment_name> exo dns add A $DOMAIN -a <service_cluster_lb_ip> -n notary.harbor.<environment_name> Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record: exo dns add A $DOMAIN -a <workload_cluster_lb_ip> -n *.<environment_name> Deploy Rook To deploy Rook, please go to the compliantkubernetes-kubespray repo root directory and run the following. cd rook for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do sops --decrypt ${ CK8S_CONFIG_PATH } /.state/kube_config_ $CLUSTER .yaml > $CLUSTER .yaml export KUBECONFIG = $CLUSTER .yaml ./deploy-rook.sh done","title":"Deploying Compliant Kubernetes clusters using Kubespray"},{"location":"operator-manual/exoscale/#deploying-compliant-kubernetes-apps","text":"Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps and the Rook storage orchestration service. If you haven't done so already, clone the compliantkubernetes-apps repo and install pre-requisites. git clone https://github.com/elastisys/compliantkubernetes-apps.git cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Initialize the apps configuration. export CK8S_ENVIRONMENT_NAME = exoscale #export CK8S_FLAVOR=[dev|prod] # defaults to dev export CK8S_CONFIG_PATH = ~/.ck8s/exoscale export CK8S_CLOUD_PROVIDER = exoscale export CK8S_PGP_FP = <your GPG key fingerprint> # retrieve with gpg --list-secret-keys ./bin/ck8s init Three files, sc-config.yaml and wc-config.yaml , and secrets.yaml , were generated in the ${CK8S_CONFIG_PATH} directory. Configure the apps. Edit the configuration files ${CK8S_CONFIG_PATH}/sc-config.yaml , ${CK8S_CONFIG_PATH}/wc-config.yaml and ${CK8S_CONFIG_PATH}/secrets.yaml and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted. vim ${ CK8S_CONFIG_PATH } /sc-config.yaml vim ${ CK8S_CONFIG_PATH } /wc-config.yaml sops ${ CK8S_CONFIG_PATH } /secrets.yaml The following are the minimum change you should perform: # ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml global: baseDomain: \"set-me\" # set to <enovironment_name>.$DOMAIN opsDomain: \"set-me\" # set to ops.<environment_name>.$DOMAIN issuer: letsencrypt-prod objectStorage: type: \"s3\" s3: region: \"set-me\" # Region for S3 buckets, e.g, west-1 regionAddress: \"set-me\" # Region address, e.g, s3.us-west-1.amazonaws.com regionEndpoint: \"set-me\" # e.g., https://s3.us-west-1.amazonaws.com storageClasses: default: rook-ceph-block nfs: enabled: false cinder: enabled: false local: enabled: false ebs: enabled: false # ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above) fluentd: forwarder: useRegionEndpoint: \"set-me\" # set it to either true or false elasticsearch: dataNode: storageClass: rook-ceph-block # ${CK8S_CONFIG_PATH}/secrets.yaml objectStorage: s3: accessKey: \"set-me\" #set to your s3 accesskey secretKey: \"set-me\" #set to your s3 secretKey Bootstrapping To deploy the Compliant Kubernetes apps, please go to the compliantkubernetes-apps repo root directory and run the following. export CK8S_CONFIG_PATH = ~/.ck8s/exoscale for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s bootstrap $CLUSTER done Installing Compliant Kubernetes apps. To deploy the Compliant Kubernetes apps, please go to the compliantkubernetes-apps repo root directory and run the following. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s apps $CLUSTER done 7. Testing: After completing the installation step you can test if the apps are properly installed and ready using the commands below. for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do ./bin/ck8s test $CLUSTER done Done. Navigate to the endpoints, for example grafana.<environment_name>.$DOMAIN , kibana.<environment_name>.$DOMAIN , harbor.<environment_name>.$DOMAIN , etc. to discover Compliant Kubernetes's features.","title":"Deploying Compliant Kubernetes Apps"},{"location":"operator-manual/exoscale/#teardown","text":"To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section). for CLUSTER in ${ SERVICE_CLUSTER } ${ WORKLOAD_CLUSTERS [@] } ; do terraform destroy \\ -var-file inventory/ $CLUSTER /default.tfvars \\ -auto-approve \\ -state = inventory/ $CLUSTER /tfstate- $CLUSTER .tfstate \\ contrib/terraform/exoscale done # Remove dns records exo dns remove $DOMAIN *.ops.<environment_name> exo dns remove $DOMAIN grafana.<environment_name> exo dns remove $DOMAIN harbor.<environment_name> exo dns remove $DOMAIN kibana.<environment_name> exo dns remove $DOMAIN dex.<environment_name> exo dns remove $DOMAIN notary.harbor.<environment_name> exo dns remove $DOMAIN *.<environment_name>","title":"Teardown"},{"location":"operator-manual/exoscale/#further-reading","text":"Elastisys Compliant Kubernetes Kubespray Kubernetes on Exoscale with Terraform Compliant Kubernetes apps repo Configurations option","title":"Further Reading"},{"location":"operator-manual/getting-started/","text":"Getting Started Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying compliantkubernetes-apps on top of them. Pre-requisites for Creating Vanilla Kubernetes clusters In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need: Terraform Ansible Ansible is best installed as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray pip3 install -r kubespray/requirements.txt Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider: AWS CLI Exoscale CLI OpenStack Client VMware vSphere CLI (govmomi) Pre-requisites for compliantkubernetes-apps Using Ansible, these can be retrieved as follows: git clone https://github.com/elastisys/compliantkubernetes-apps cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml Misc Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows: ssh-keygen Configuration secrets in Compliant Kubernetes are encrypted using SOPS . We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows: gpg --full-generate-key","title":"Getting Started"},{"location":"operator-manual/getting-started/#getting-started","text":"Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying compliantkubernetes-apps on top of them.","title":"Getting Started"},{"location":"operator-manual/getting-started/#pre-requisites-for-creating-vanilla-kubernetes-clusters","text":"In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need: Terraform Ansible Ansible is best installed as follows: git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray cd compliantkubernetes-kubespray pip3 install -r kubespray/requirements.txt Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider: AWS CLI Exoscale CLI OpenStack Client VMware vSphere CLI (govmomi)","title":"Pre-requisites for Creating Vanilla Kubernetes clusters"},{"location":"operator-manual/getting-started/#pre-requisites-for-compliantkubernetes-apps","text":"Using Ansible, these can be retrieved as follows: git clone https://github.com/elastisys/compliantkubernetes-apps cd compliantkubernetes-apps ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127 .0.0.1, get-requirements.yaml","title":"Pre-requisites for compliantkubernetes-apps"},{"location":"operator-manual/getting-started/#misc","text":"Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows: ssh-keygen Configuration secrets in Compliant Kubernetes are encrypted using SOPS . We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows: gpg --full-generate-key","title":"Misc"},{"location":"operator-manual/ingress/","text":"Ingress Compliant Kubernetes (CK8S) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8S can configure the Ingress controller in two different ways depending on the underlying infrastructure. Using a Service of type LoadBalancer When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer. Note This is only currently supported in CK8S for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default Using the host network For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service. On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns. For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available. Installation The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: ingressNginx : useHostPort : \"\" service : enabled : \"\" type : \"\" If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults. For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy Ingress resource The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation .","title":"Ingress"},{"location":"operator-manual/ingress/#ingress","text":"Compliant Kubernetes (CK8S) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8S can configure the Ingress controller in two different ways depending on the underlying infrastructure.","title":"Ingress"},{"location":"operator-manual/ingress/#using-a-service-of-type-loadbalancer","text":"When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer. Note This is only currently supported in CK8S for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default","title":"Using a Service of type LoadBalancer"},{"location":"operator-manual/ingress/#using-the-host-network","text":"For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service. On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns. For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available.","title":"Using the host network"},{"location":"operator-manual/ingress/#installation","text":"The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: ingressNginx : useHostPort : \"\" service : enabled : \"\" type : \"\" If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults. For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy","title":"Installation"},{"location":"operator-manual/ingress/#ingress-resource","text":"The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation .","title":"Ingress resource"},{"location":"operator-manual/overview/","text":"Work in progress Please check back soon!","title":"Overview"},{"location":"operator-manual/overview/#work-in-progress","text":"Please check back soon!","title":"Work in progress"},{"location":"user-guide/","text":"User Guide Overview This guide is for users who manage application on top of Compliant Kubernetes. Note Please make sure you've completed the getting started guide . A user can be described via the following user stories: As a Continuous Delivery (CD) pipeline I want to push changes to the Compliant Kubernetes cluster, so that the new version of the application is running. As an application developer, I want to inspect how my application is running, so that I can take better development decisions. Note We suggest application developers to only perform changes to a production Compliant Kubernetes cluster via a Continuous Delivery Pipeline. This method, also known as GitOps, provides an audit log, review and testing of system changes for \"free\". This significantly facilitates complying with change management policies.","title":"Overview"},{"location":"user-guide/#user-guide-overview","text":"This guide is for users who manage application on top of Compliant Kubernetes. Note Please make sure you've completed the getting started guide . A user can be described via the following user stories: As a Continuous Delivery (CD) pipeline I want to push changes to the Compliant Kubernetes cluster, so that the new version of the application is running. As an application developer, I want to inspect how my application is running, so that I can take better development decisions. Note We suggest application developers to only perform changes to a production Compliant Kubernetes cluster via a Continuous Delivery Pipeline. This method, also known as GitOps, provides an audit log, review and testing of system changes for \"free\". This significantly facilitates complying with change management policies.","title":"User Guide Overview"},{"location":"user-guide/backup/","text":"Backups Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that are relevant to backups are: Annex 12 , article A.12.3.1 \"Information Backup\". What is Velero? Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything. Usage Velero is deployed in both the workload cluster and the service cluster. Following are instructions for backing up and restoring resources. Backing up Velero takes a daily backup of all Kubernetes Resources with the label velero: backup . Persistent Volumes will be backed up if they are tied to a Pod with the previously mentioned label and if that Pod is annotated with backup.velero.io/backup-volumes=<volume1>,<volume2>,... , where the value is a comma separated list of the volume names. For user applications deployed in the workload cluster, make sure to add these labels and annotations to the resources that need to be backed up. In the service cluster, Grafana (and its associated Persistent Volume) is configured to be backed up. Backups are stored for 720 hours (30 days). Restoring Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore the state from the latest daily backup, first download the Velero cli: https://github.com/vmware-tanzu/velero/releases/tag/v1.1.0 (version 1.1.0). Then, to restore your Resources from the backup, run: velero restore create --from-schedule velero-daily-backup -w This command will wait until the restore has finished. Make sure that KUBECONFIG is exported as an environment variable when you run the restore command. Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behaviour, then the volume must be wiped manually before restoring. To restore the service cluster from a Velero backup, set restore.velero in your {CK8S_CONFIG_PATH}/sc-config.yaml to true , and then reapply the service cluster apps (in compliantkubernetes-apps : bin/ck8s apply sc ). By default, the latest daily backup is chosen; to restore from a different backup, set restore.veleroBackupName to the desired backup name.","title":"Backups"},{"location":"user-guide/backup/#backups","text":"Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead.","title":"Backups"},{"location":"user-guide/backup/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that are relevant to backups are: Annex 12 , article A.12.3.1 \"Information Backup\".","title":"Compliance needs"},{"location":"user-guide/backup/#what-is-velero","text":"Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything.","title":"What is Velero?"},{"location":"user-guide/backup/#usage","text":"Velero is deployed in both the workload cluster and the service cluster. Following are instructions for backing up and restoring resources.","title":"Usage"},{"location":"user-guide/backup/#backing-up","text":"Velero takes a daily backup of all Kubernetes Resources with the label velero: backup . Persistent Volumes will be backed up if they are tied to a Pod with the previously mentioned label and if that Pod is annotated with backup.velero.io/backup-volumes=<volume1>,<volume2>,... , where the value is a comma separated list of the volume names. For user applications deployed in the workload cluster, make sure to add these labels and annotations to the resources that need to be backed up. In the service cluster, Grafana (and its associated Persistent Volume) is configured to be backed up. Backups are stored for 720 hours (30 days).","title":"Backing up"},{"location":"user-guide/backup/#restoring","text":"Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first. To restore the state from the latest daily backup, first download the Velero cli: https://github.com/vmware-tanzu/velero/releases/tag/v1.1.0 (version 1.1.0). Then, to restore your Resources from the backup, run: velero restore create --from-schedule velero-daily-backup -w This command will wait until the restore has finished. Make sure that KUBECONFIG is exported as an environment variable when you run the restore command. Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behaviour, then the volume must be wiped manually before restoring. To restore the service cluster from a Velero backup, set restore.velero in your {CK8S_CONFIG_PATH}/sc-config.yaml to true , and then reapply the service cluster apps (in compliantkubernetes-apps : bin/ck8s apply sc ). By default, the latest daily backup is chosen; to restore from a different backup, set restore.veleroBackupName to the desired backup name.","title":"Restoring"},{"location":"user-guide/dashboard/","text":"Compliant Kubernetes (CK8s) Dashboard To facilitate continuous compliance, Compliant Kubernetes comes with a \"single pane of glass\" to present all aspects of the cluster that are relevant for internal or external auditing. The CK8s Dashboard provides different visualizations regarding the state of the cluster. It has a set of panels organized and arranged into one or more rows. Each panel visualizes different aspect of the system. The different visualizations can be broadly divided into the following five categories. Cluster: Information regarding the state of the state Falco: Runtime security threat monitoring and detection Grafana: Metrics visualization and analytics Elastic: Log visualization and analytics Harbor: Manage and serve container images in a secure environment Authenticating You should have received the URL of the dashboard from your Compliant Kubernetes operator. Otherwise, simply contact support. After typing the URL, you will be presented with a window for choosing the login method: Choose \"Log in with dex\". You will see the image below: Depending on how Compliant Kubernetes is setup for you, click on the Identity Provider you prefer, or login using a statically configured username and password. Home Screen Once logged into the dashboard, you will get the main page similar to the figure below. The left panel in the figure shows the link to the five categories: cluster Falco Grafana Elastic Harbor The central part presents different panels with summary information, e.g., number of Nodes, Pods, CPU and RAM usage, about the cluster. Cluster: How to get detail information about the cluster To get information about the state of the cluster, please click the Cluster icon from the left panel in the main page. You will get a page similar to the one shown below. The cluster dashboard contains three tabs, namely Cluster , Namespaces and Nodes which are located on the top right side aligned horizontally (i.e., labeled Tabs in the figure). By default the Cluster tab is selected when you click the cluster icon from the main left panel. To distinguish the selected tab from the rest the color of the selected tab label is blue along with blue underline. The Cluster tab has four different panels displaying aggregate information about Nodes , Pods , Pods CPU Use and Pods RAM use . The Namespaces tab provides information about the different namespaces in the cluster. To get information about namespaces: Select the Namespaces tab under cluster dashboard. A panel with list of namespaces in the cluster with their status and how long they have been created will be displayed. The panel looks like the figure below. To get different metric information about each namespace, hover your mouse on the options dropdown menu on the top left side under Namespaces tab (i.e., labeled capabilities in the figure above) and click Open in Grafana . You will be redirected to grafana website. By default different visualizations about the first namespace, argocd in this example, will be loaded by Grafana. You can change the namespace you if want to see for a different namespace. The following figure shows one of the visualizations loaded by grafana. To get further information about each namespace, click on the namespace you want to get more information. Depending on your privilege, you will get one of the following pages. If you do not have privilege, you will get the following error message If you have privilege to access the namespace, you will get the following page with different panels. As you can see in the figure, the different panels provide detail as well as aggregate information about Pods , CPUs and RAM for the selected namespace. If you want to zoom in further and get more information about pods, you can select one pod at a time and get a detailed information. Falco: How to get runtime security issues Falco is a cloud-native runtime security system that monitors file changes, network activity, the process table, and other data for suspicious behavior. It inspects events at the system call level of a host through a kernel module or an extended BPF probe. Falco contains a rich set of rules that you can edit for flagging specific abnormal behaviors and for creating allow lists for normal computer operations. To get information about security issues, if any, please click the Falco icon from the left panel in the main page. You will get a page similar to the one shown below. For more details, see Falco . Grafana: Metrics visualization and analysis Grafana is used to provide visualizations about the system based on different metrics collected from the system. To get such visualization, please click the Grafana icon from the left panel in the main page. You will be redirected to grafana website. You will land to a page similar to the figure shown below. To learn more on how to explore the Grafana dashboard, please visit the official Grafana website. Elasticsearch: Log visualization and analysis Open Distro for Elasticsearch is used for logs visualization and analytics. To visualization and analytics logs, please click the Elastic icon from the left panel in the main page. You will be redirected to Kibana website. For more details, see Elasticsearch/Kibana . Harbor: private container registry Harbor is used to manage and scan container images to ensure that there no any vulnerability. To access Harbor, please click the Harbor icon from the left panel in the main page. You will be redirected to Harbor website. For more details, see Harbor .","title":"Dashboard"},{"location":"user-guide/dashboard/#compliant-kubernetes-ck8s-dashboard","text":"To facilitate continuous compliance, Compliant Kubernetes comes with a \"single pane of glass\" to present all aspects of the cluster that are relevant for internal or external auditing. The CK8s Dashboard provides different visualizations regarding the state of the cluster. It has a set of panels organized and arranged into one or more rows. Each panel visualizes different aspect of the system. The different visualizations can be broadly divided into the following five categories. Cluster: Information regarding the state of the state Falco: Runtime security threat monitoring and detection Grafana: Metrics visualization and analytics Elastic: Log visualization and analytics Harbor: Manage and serve container images in a secure environment","title":"Compliant Kubernetes (CK8s) Dashboard"},{"location":"user-guide/dashboard/#authenticating","text":"You should have received the URL of the dashboard from your Compliant Kubernetes operator. Otherwise, simply contact support. After typing the URL, you will be presented with a window for choosing the login method: Choose \"Log in with dex\". You will see the image below: Depending on how Compliant Kubernetes is setup for you, click on the Identity Provider you prefer, or login using a statically configured username and password.","title":"Authenticating"},{"location":"user-guide/dashboard/#home-screen","text":"Once logged into the dashboard, you will get the main page similar to the figure below. The left panel in the figure shows the link to the five categories: cluster Falco Grafana Elastic Harbor The central part presents different panels with summary information, e.g., number of Nodes, Pods, CPU and RAM usage, about the cluster.","title":"Home Screen"},{"location":"user-guide/dashboard/#cluster-how-to-get-detail-information-about-the-cluster","text":"To get information about the state of the cluster, please click the Cluster icon from the left panel in the main page. You will get a page similar to the one shown below. The cluster dashboard contains three tabs, namely Cluster , Namespaces and Nodes which are located on the top right side aligned horizontally (i.e., labeled Tabs in the figure). By default the Cluster tab is selected when you click the cluster icon from the main left panel. To distinguish the selected tab from the rest the color of the selected tab label is blue along with blue underline. The Cluster tab has four different panels displaying aggregate information about Nodes , Pods , Pods CPU Use and Pods RAM use . The Namespaces tab provides information about the different namespaces in the cluster. To get information about namespaces: Select the Namespaces tab under cluster dashboard. A panel with list of namespaces in the cluster with their status and how long they have been created will be displayed. The panel looks like the figure below. To get different metric information about each namespace, hover your mouse on the options dropdown menu on the top left side under Namespaces tab (i.e., labeled capabilities in the figure above) and click Open in Grafana . You will be redirected to grafana website. By default different visualizations about the first namespace, argocd in this example, will be loaded by Grafana. You can change the namespace you if want to see for a different namespace. The following figure shows one of the visualizations loaded by grafana. To get further information about each namespace, click on the namespace you want to get more information. Depending on your privilege, you will get one of the following pages. If you do not have privilege, you will get the following error message If you have privilege to access the namespace, you will get the following page with different panels. As you can see in the figure, the different panels provide detail as well as aggregate information about Pods , CPUs and RAM for the selected namespace. If you want to zoom in further and get more information about pods, you can select one pod at a time and get a detailed information.","title":"Cluster: How to get detail information about the cluster"},{"location":"user-guide/dashboard/#falco-how-to-get-runtime-security-issues","text":"Falco is a cloud-native runtime security system that monitors file changes, network activity, the process table, and other data for suspicious behavior. It inspects events at the system call level of a host through a kernel module or an extended BPF probe. Falco contains a rich set of rules that you can edit for flagging specific abnormal behaviors and for creating allow lists for normal computer operations. To get information about security issues, if any, please click the Falco icon from the left panel in the main page. You will get a page similar to the one shown below. For more details, see Falco .","title":"Falco: How to get runtime security issues"},{"location":"user-guide/dashboard/#grafana-metrics-visualization-and-analysis","text":"Grafana is used to provide visualizations about the system based on different metrics collected from the system. To get such visualization, please click the Grafana icon from the left panel in the main page. You will be redirected to grafana website. You will land to a page similar to the figure shown below. To learn more on how to explore the Grafana dashboard, please visit the official Grafana website.","title":"Grafana: Metrics visualization and analysis"},{"location":"user-guide/dashboard/#elasticsearch-log-visualization-and-analysis","text":"Open Distro for Elasticsearch is used for logs visualization and analytics. To visualization and analytics logs, please click the Elastic icon from the left panel in the main page. You will be redirected to Kibana website. For more details, see Elasticsearch/Kibana .","title":"Elasticsearch: Log visualization and analysis"},{"location":"user-guide/dashboard/#harbor-private-container-registry","text":"Harbor is used to manage and scan container images to ensure that there no any vulnerability. To access Harbor, please click the Harbor icon from the left panel in the main page. You will be redirected to Harbor website. For more details, see Harbor .","title":"Harbor: private container registry"},{"location":"user-guide/elasticsearch/","text":"Logging Compliant Kubernetes (CK8S) provides the mechanism to manage your cluster as well as the lifecycle of thousands of containerized applications deployed in the cluster. The resources managed by CK8S are expected to be highly distributed with dynamic behaviors. An instance of CK8S cluster environment involves several components with nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads. When dealing with a large pool of containerized applications and workloads in CK8S, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that mostly concerns logging are: Annex 12 , article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\". Annex 16 which deals with incident management. In Compliant Kubernetes, Elasticsearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\". Open Distro for Elasticsearch Raw logs in CK8S are normalized, filtered, and processed by fluentd and shipped to Elasticsearch for storage and analysis. CK8S uses fully open source version of Elasticsearch called Open Distro for Elasticsearch . Open Distro for Elasticsearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. Kibana is used as visualization and analysis interface for Elasticsearch for all your logs. Visualization using Kibana Kibana is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. When you log into Kibana, you will encounter a page similar to the one shown below. Since we are concerned with searching logs and their visualization, we will focus on Visualize and Explore Data as indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official Kibana documentation . Before we dive into Kibana, let us discuss the type of logs ingested into Elasticsearch. Logs in CK8S cluster are filtered and indexed by fluentd into three categories: kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Kubernetes logs that provide insight into CK8S resources such as nodes, Pods, containers, deployments and replica sets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8S ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Others logs other than the above two are indexed and shipped to Elasticsearch as others . Such logs are basically your application level logs. This is mostly related to the ISO 27001 requirement A.12.4.1 \"Event Logging\". Let us dive into Kibana then. Data Visualization and Exploration in Kibana As you can see in the figure above, data visualzation and expoloration in Kibana has three components: Discover , Visualize and Dashboard . The following section describes each components using examples. Discover The discover component in Kibana is used for exploring, searching and filtering logs. Click Discover in the main Kibana page to access the features provided by it. The figure below shows partial view of the page that you will get under Discover . As yopu can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category . To appreciate Kibana's searching and filtering capability, let us get data for the following question: Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound We can use different ways to find the answer for the question. Below is one possible solution. Write sourceIPs: 172.16.0.3 in the search textbox . Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save . The following figure shows the details. To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update . The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time. Once you are done, you will see a result similar to the following figure. Visualize The Visualize component in Kibana is to create different visualizations. Let us create a couple of visualizations. To create visualizations: Go to the main Kibana page and click Visual . Click Create visualization link located on the top right side of the page. Select a visualization type, we will use Pie here. Choose the index name or saved query name, if any, under NNew Pie / Choose a source . We will use the Kubernetes index here. By default a pie chart with the total number of logs will be provided by kibana. Let us divide the pie chart based on the number of logs contributed by each namespace . To do that perform the following steps: Under Buckets click add then Split Slices . See the figure below. Under aggregation select Significant Terms terms. see the figure below. Select Kubernetes.namespace_name.keyword under field . See the figure below. The final result will look like the following figure. Please save the pie chart as we will use it later. Let us create a similar pie chart using host instead of namespace . The chart will look like the following figure. Dashboard The Dashboard component in Kibana is used for organizing related visualizations together. Let us bring the two visualizations that we created above together in a single dashboard. To do that: Go to the main Kibana page and click Dashboard . Click Create Dashboard link located on the top right side of the page. Click Add existing link located on the left side. Select the name of the two charts/visualizations that you created above. The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page. Accessing Falco and OPA Logs To access Falco or OPA logs, go to the Discover panel and write Falco or OPA on the search textbox . Make sure that the Kubernetes log index category is selected. The figure below shows the search result for Falco logs. The figure below shows the search result for OPA logs. Further Reading Open Distro for Elasticsearch Kibana Open Distro for Elasticsearch \u2013 How Different Is It? Fluentd","title":"Elasticsearch/Kibana"},{"location":"user-guide/elasticsearch/#logging","text":"Compliant Kubernetes (CK8S) provides the mechanism to manage your cluster as well as the lifecycle of thousands of containerized applications deployed in the cluster. The resources managed by CK8S are expected to be highly distributed with dynamic behaviors. An instance of CK8S cluster environment involves several components with nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads. When dealing with a large pool of containerized applications and workloads in CK8S, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity.","title":"Logging"},{"location":"user-guide/elasticsearch/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 . The annexes that mostly concerns logging are: Annex 12 , article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\". Annex 16 which deals with incident management. In Compliant Kubernetes, Elasticsearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\".","title":"Compliance needs"},{"location":"user-guide/elasticsearch/#open-distro-for-elasticsearch","text":"Raw logs in CK8S are normalized, filtered, and processed by fluentd and shipped to Elasticsearch for storage and analysis. CK8S uses fully open source version of Elasticsearch called Open Distro for Elasticsearch . Open Distro for Elasticsearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. Kibana is used as visualization and analysis interface for Elasticsearch for all your logs.","title":"Open Distro for Elasticsearch"},{"location":"user-guide/elasticsearch/#visualization-using-kibana","text":"Kibana is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. When you log into Kibana, you will encounter a page similar to the one shown below. Since we are concerned with searching logs and their visualization, we will focus on Visualize and Explore Data as indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official Kibana documentation . Before we dive into Kibana, let us discuss the type of logs ingested into Elasticsearch. Logs in CK8S cluster are filtered and indexed by fluentd into three categories: kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Kubernetes logs that provide insight into CK8S resources such as nodes, Pods, containers, deployments and replica sets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8S ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\". Others logs other than the above two are indexed and shipped to Elasticsearch as others . Such logs are basically your application level logs. This is mostly related to the ISO 27001 requirement A.12.4.1 \"Event Logging\". Let us dive into Kibana then.","title":"Visualization using Kibana"},{"location":"user-guide/elasticsearch/#data-visualization-and-exploration-in-kibana","text":"As you can see in the figure above, data visualzation and expoloration in Kibana has three components: Discover , Visualize and Dashboard . The following section describes each components using examples.","title":"Data Visualization and Exploration in Kibana"},{"location":"user-guide/elasticsearch/#discover","text":"The discover component in Kibana is used for exploring, searching and filtering logs. Click Discover in the main Kibana page to access the features provided by it. The figure below shows partial view of the page that you will get under Discover . As yopu can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category . To appreciate Kibana's searching and filtering capability, let us get data for the following question: Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound We can use different ways to find the answer for the question. Below is one possible solution. Write sourceIPs: 172.16.0.3 in the search textbox . Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save . The following figure shows the details. To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update . The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time. Once you are done, you will see a result similar to the following figure.","title":"Discover"},{"location":"user-guide/elasticsearch/#visualize","text":"The Visualize component in Kibana is to create different visualizations. Let us create a couple of visualizations. To create visualizations: Go to the main Kibana page and click Visual . Click Create visualization link located on the top right side of the page. Select a visualization type, we will use Pie here. Choose the index name or saved query name, if any, under NNew Pie / Choose a source . We will use the Kubernetes index here. By default a pie chart with the total number of logs will be provided by kibana. Let us divide the pie chart based on the number of logs contributed by each namespace . To do that perform the following steps: Under Buckets click add then Split Slices . See the figure below. Under aggregation select Significant Terms terms. see the figure below. Select Kubernetes.namespace_name.keyword under field . See the figure below. The final result will look like the following figure. Please save the pie chart as we will use it later. Let us create a similar pie chart using host instead of namespace . The chart will look like the following figure.","title":"Visualize"},{"location":"user-guide/elasticsearch/#dashboard","text":"The Dashboard component in Kibana is used for organizing related visualizations together. Let us bring the two visualizations that we created above together in a single dashboard. To do that: Go to the main Kibana page and click Dashboard . Click Create Dashboard link located on the top right side of the page. Click Add existing link located on the left side. Select the name of the two charts/visualizations that you created above. The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page.","title":"Dashboard"},{"location":"user-guide/elasticsearch/#accessing-falco-and-opa-logs","text":"To access Falco or OPA logs, go to the Discover panel and write Falco or OPA on the search textbox . Make sure that the Kubernetes log index category is selected. The figure below shows the search result for Falco logs. The figure below shows the search result for OPA logs.","title":"Accessing Falco and OPA Logs"},{"location":"user-guide/elasticsearch/#further-reading","text":"Open Distro for Elasticsearch Kibana Open Distro for Elasticsearch \u2013 How Different Is It? Fluentd","title":"Further Reading"},{"location":"user-guide/harbor/","text":"Harbor - private container registry This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden. What is a container registry and why it is needed? A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry. A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository. What is Harbor? Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project , proving that it is widely used and is well supported. Why is Harbor used in Compliant Kubernetes? Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to harbor, the default scanner is Trivy. Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning. In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images. If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with kubectl describe you will find an error message similar to this: Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist. By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this: for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"] Further reading For more information please refer to the official Harbor , Trivy , Open Policy Agent and Gatekeeper documentation.","title":"Harbor"},{"location":"user-guide/harbor/#harbor-private-container-registry","text":"This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden.","title":"Harbor - private container registry"},{"location":"user-guide/harbor/#what-is-a-container-registry-and-why-it-is-needed","text":"A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry. A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository.","title":"What is a container registry and why it is needed?"},{"location":"user-guide/harbor/#what-is-harbor","text":"Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project , proving that it is widely used and is well supported.","title":"What is Harbor?"},{"location":"user-guide/harbor/#why-is-harbor-used-in-compliant-kubernetes","text":"Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to harbor, the default scanner is Trivy. Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning. In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images. If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with kubectl describe you will find an error message similar to this: Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist. By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this: for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"]","title":"Why is Harbor used in Compliant Kubernetes?"},{"location":"user-guide/harbor/#further-reading","text":"For more information please refer to the official Harbor , Trivy , Open Policy Agent and Gatekeeper documentation.","title":"Further reading"},{"location":"user-guide/kubernetes-api/","text":"Kubernetes API The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API. The following sections describe how to access the cluster in order to manage your Kubernetes resources. Authentication and Access Control in Compliant Kubernetes In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex . Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords. The authorization is done by the Kubernetes API based on Kubernetes role-based access controls . Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege , you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege. Note Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster. Usage guide This section focuses on using the kubeconfig. Using the kubeconfig file The kubeconfig file can be used with kubectl by: Setting and exporting the KUBECONFIG environment variable: Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files . Authenticating to the Kubernetes API To authenticate to the Kubernetes API, run a kubectl command. The oidc-login plugin will launch a browser where you log in to the cluster: This page contains the authentication options provided by your administrator. Select your log in method and log in: Once you have logged in through the browser, you are authenticated to the cluster: Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources! Further reading dex on GitHub oidc-login/kubelogin on GitHub Organizing Cluster Access Using kubeconfig Files","title":"Kubernetes API"},{"location":"user-guide/kubernetes-api/#kubernetes-api","text":"The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API. The following sections describe how to access the cluster in order to manage your Kubernetes resources.","title":"Kubernetes API"},{"location":"user-guide/kubernetes-api/#authentication-and-access-control-in-compliant-kubernetes","text":"In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex . Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords. The authorization is done by the Kubernetes API based on Kubernetes role-based access controls . Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege , you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege. Note Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster.","title":"Authentication and Access Control in Compliant Kubernetes"},{"location":"user-guide/kubernetes-api/#usage-guide","text":"This section focuses on using the kubeconfig.","title":"Usage guide"},{"location":"user-guide/kubernetes-api/#using-the-kubeconfig-file","text":"The kubeconfig file can be used with kubectl by: Setting and exporting the KUBECONFIG environment variable: Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files .","title":"Using the kubeconfig file"},{"location":"user-guide/kubernetes-api/#authenticating-to-the-kubernetes-api","text":"To authenticate to the Kubernetes API, run a kubectl command. The oidc-login plugin will launch a browser where you log in to the cluster: This page contains the authentication options provided by your administrator. Select your log in method and log in: Once you have logged in through the browser, you are authenticated to the cluster: Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources!","title":"Authenticating to the Kubernetes API"},{"location":"user-guide/kubernetes-api/#further-reading","text":"dex on GitHub oidc-login/kubelogin on GitHub Organizing Cluster Access Using kubeconfig Files","title":"Further reading"},{"location":"user-guide/prometheus/","text":"Prometheus and Grafana This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden. Why Prometheus and Grafana? Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus. Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member. Compliance needs The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12 , article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management. Capacity management Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\" Promethus and Grafana helps with this as the resource usage, such as storage capacity, cpu, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements. The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\" Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded. Incident management Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future. Prometheus and Grafana can help with this by making it easier to: collect evidence as soon as possible after the occurrence. conduct an information security forensics analysis communicate the existence of the information security incident or any relevant details to the leadership. Prometheus and Grafana in Compliant Kubernetes Prometheus Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default. crd apigroup kind used by description alertmanagers monitoring.coreos.com Alertmanager prometheus-alerts podmonitors monitoring.coreos.com PodMonitor customer-rbac prometheuses monitoring.coreos.com Prometheus prometheusrules monitoring.coreos.com PrometheusRule customer-rbac, elasticsearch servicemonitors monitoring.coreos.com ServiceMonitor customer-rbac, dex, grafana, kibana, elastisearch, influxdb thanosrulers monitoring.coreos.com ThanosRuler Accessing Prometheus The web interface is not exposed by default in Compliant Kubernetes. In order to access it, the most straight-forward way is to use port forwarding via the Kubernetes API . kubectl -- -n monitoring port-forward prometheus-prometheus-operator-prometheus-0 9090:9090 Depending on your Compliant Kubernetes settings, access to the Prometheus server might have been disabled by the operator. Grafana Grafana can be accessed at the endpoint provided by the compliant kubernetes install scripts. If you have configured dex you can login with a connected account. Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (for squares) at the lefthand side of the grafana window and selecting Manage. Some examples of useful dashboards are listed below. Node health The Nodes dashboard (Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Pod health The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs. Further reading For more information please refer to the official Prometheus and Grafana documentation.","title":"Prometheus/Grafana"},{"location":"user-guide/prometheus/#prometheus-and-grafana","text":"This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden.","title":"Prometheus and Grafana"},{"location":"user-guide/prometheus/#why-prometheus-and-grafana","text":"Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus. Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member.","title":"Why Prometheus and Grafana?"},{"location":"user-guide/prometheus/#compliance-needs","text":"The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12 , article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management.","title":"Compliance needs"},{"location":"user-guide/prometheus/#capacity-management","text":"Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\" Promethus and Grafana helps with this as the resource usage, such as storage capacity, cpu, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements. The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\" Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded.","title":"Capacity management"},{"location":"user-guide/prometheus/#incident-management","text":"Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future. Prometheus and Grafana can help with this by making it easier to: collect evidence as soon as possible after the occurrence. conduct an information security forensics analysis communicate the existence of the information security incident or any relevant details to the leadership.","title":"Incident management"},{"location":"user-guide/prometheus/#prometheus-and-grafana-in-compliant-kubernetes","text":"","title":"Prometheus and Grafana in Compliant Kubernetes"},{"location":"user-guide/prometheus/#prometheus","text":"Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default. crd apigroup kind used by description alertmanagers monitoring.coreos.com Alertmanager prometheus-alerts podmonitors monitoring.coreos.com PodMonitor customer-rbac prometheuses monitoring.coreos.com Prometheus prometheusrules monitoring.coreos.com PrometheusRule customer-rbac, elasticsearch servicemonitors monitoring.coreos.com ServiceMonitor customer-rbac, dex, grafana, kibana, elastisearch, influxdb thanosrulers monitoring.coreos.com ThanosRuler","title":"Prometheus"},{"location":"user-guide/prometheus/#accessing-prometheus","text":"The web interface is not exposed by default in Compliant Kubernetes. In order to access it, the most straight-forward way is to use port forwarding via the Kubernetes API . kubectl -- -n monitoring port-forward prometheus-prometheus-operator-prometheus-0 9090:9090 Depending on your Compliant Kubernetes settings, access to the Prometheus server might have been disabled by the operator.","title":"Accessing Prometheus"},{"location":"user-guide/prometheus/#grafana","text":"Grafana can be accessed at the endpoint provided by the compliant kubernetes install scripts. If you have configured dex you can login with a connected account. Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (for squares) at the lefthand side of the grafana window and selecting Manage. Some examples of useful dashboards are listed below.","title":"Grafana"},{"location":"user-guide/prometheus/#node-health","text":"The Nodes dashboard (Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.","title":"Node health"},{"location":"user-guide/prometheus/#pod-health","text":"The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.","title":"Pod health"},{"location":"user-guide/prometheus/#further-reading","text":"For more information please refer to the official Prometheus and Grafana documentation.","title":"Further reading"}]}